{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load the model from the file \"sbm3x3_pclam_roc_0.210_auc_0.860\"\n",
    "import torch\n",
    "from torch_geometric.utils import dropout_node, contains_isolated_nodes, is_undirected, contains_self_loops, is_undirected, to_undirected, k_hop_subgraph, coalesce, subgraph, to_dense_adj\n",
    "from torch.autograd import grad\n",
    "from torch_geometric.transforms import TwoHop\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "from transformation import RealNVP\n",
    "import transformation as tr\n",
    "from datasets.import_dataset import import_dataset, load_data_GGAD\n",
    "import clamiter as ci\n",
    "from utils import utils\n",
    "from utils.plotting import *\n",
    "from trainer import Trainer\n",
    "from tests import test_no_duplicacy, test_get_fat_ds_test, test_densify_test_from_train\n",
    "from scripts.scripting_utils import print_prior_training_stats\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Config!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/danny/AAAI_pieclam/tests/../datasets/import_dataset.py:410: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  edge_index = torch.tensor(adj.nonzero(), dtype=torch.long)\n",
      "/home/user/anaconda3/envs/piegam/lib/python3.11/site-packages/torch_geometric/edge_index.py:784: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:415:::  \n",
      " starting optimization of piegam on ellipticGGAD on device cuda\n",
      "\n",
      " configs_dict: \n",
      "{\n",
      "    \"clamiter_init\": {\n",
      "        \"dim_feat\": 30,\n",
      "        \"dim_attr\": 93,\n",
      "        \"s_reg\": 0.0,\n",
      "        \"l1_reg\": 1,\n",
      "        \"T\": 1,\n",
      "        \"hidden_dim\": 64,\n",
      "        \"num_coupling_blocks\": 32,\n",
      "        \"num_layers_mlp\": 2\n",
      "    },\n",
      "    \"feat_opt\": {\n",
      "        \"lr\": 3e-06,\n",
      "        \"n_iter\": 500,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"prior_opt\": {\n",
      "        \"n_iter\": 1300,\n",
      "        \"lr\": 2e-06,\n",
      "        \"noise_amp\": 0.05,\n",
      "        \"weight_decay\": 0.1,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"back_forth\": {\n",
      "        \"n_back_forth\": 5,\n",
      "        \"scheduler_step_size\": 1,\n",
      "        \"scheduler_gamma\": 0.5,\n",
      "        \"early_stop_fit\": 0,\n",
      "        \"first_func_in_fit\": \"fit_feats\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:420:::  \n",
      " train_model_on_params, initializing feats with small_gaus\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:428:::  \n",
      " init_node_feats took 0.004324436187744141 seconds\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:499:::  \n",
      "fit, task='anomaly'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:542:::  \n",
      "in fit,\n",
      "first_func_in_fit='fit_feats'\n",
      "second_function_name='fit_prior'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 1/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 1/5 took 100.81998777389526 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.435292742452806 prior: 0.6473686626638098 prior_star: 0.46114746695738007 \n",
      "Best:\n",
      "vanilla_star: 0.49424525939670677 at iteration 19\n",
      "prior: 0.6473686626638098 at iteration 1799\n",
      "prior_star: 0.4929579610888542 at iteration 19\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 3e-06 to 1.5e-06\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.05 to 0.025\n",
      "prior lr from 2e-06 to 1e-06\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 2/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 2/5 took 100.06846046447754 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.43538192056016845 prior: 0.640072581701928 prior_star: 0.5200512737225944 \n",
      "Best:\n",
      "vanilla_star: 0.49424525939670677 at iteration 19\n",
      "prior: 0.6653647126064435 at iteration 2599\n",
      "prior_star: 0.5200512737225944 at iteration 3599\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 1.5e-06 to 7.5e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.025 to 0.0125\n",
      "prior lr from 1e-06 to 5e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 3/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 3/5 took 100.22477960586548 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.4351804333782328 prior: 0.634787671122377 prior_star: 0.5424461372372282 \n",
      "Best:\n",
      "vanilla_star: 0.49424525939670677 at iteration 19\n",
      "prior: 0.6653647126064435 at iteration 2599\n",
      "prior_star: 0.5424461372372282 at iteration 5399\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 7.5e-07 to 3.75e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.0125 to 0.00625\n",
      "prior lr from 5e-07 to 2.5e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 4/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 4/5 took 100.44649600982666 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.4349778490276674 prior: 0.6291431303682291 prior_star: 0.5490794620343484 \n",
      "Best:\n",
      "vanilla_star: 0.49424525939670677 at iteration 19\n",
      "prior: 0.6653647126064435 at iteration 2599\n",
      "prior_star: 0.5490794620343484 at iteration 7199\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 3.75e-07 to 1.875e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.00625 to 0.003125\n",
      "prior lr from 2.5e-07 to 1.25e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 5/5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# also no early stop none of that!\n",
    "# come on man!!\n",
    "#TODO: make the optimization according to the mighty and individual configs per data. \n",
    "#TODO: rearange the individual configs so that they are arranged per dataset\n",
    "\n",
    "#? TESTS: test the distance with the d to see that the kwargs thing works as expected\n",
    "\n",
    "\n",
    "# you want to print the anomay stuff all of the time!!\n",
    "model_name = 'piegam'\n",
    "ds_name = 'ellipticGGAD'\n",
    "#! problem with something to do with attr with reddit and elliptic\n",
    "ds = import_dataset(ds_name)\n",
    "fat_ds = TwoHop()(ds)\n",
    "fat_ds.edge_attr = torch.ones(fat_ds.edge_index.shape[1]).bool()\n",
    "\n",
    "if ds_name in ['Flickr', 'ACM', 'BlogCatalog']:\n",
    "    ds_to_use = ds\n",
    "elif ds_name in ['redditGGAD', 'photoGGAD', 'ellipticGGAD']:\n",
    "    ds_to_use = fat_ds\n",
    "else:\n",
    "    raise ValueError('ds_name not recognized')\n",
    "\n",
    "\n",
    "losseses = []\n",
    "acc_testses = []\n",
    "acc_valses = []\n",
    "for first_func in ['fit_feats']:\n",
    "    for model_name in ['piegam']:\n",
    "        config_triplets = [\n",
    "            ['clamiter_init', 'dim_feat', 30],\n",
    "            ['clamiter_init', 'dim_attr', 100],\n",
    "            # ['feat_opt','n_iter', 100],\n",
    "            # ['prior_opt', 'n_iter', 200],\n",
    "            ['feat_opt', 'lr', 3e-6],\n",
    "            ['prior_opt', 'lr', 2e-6],\n",
    "            ['prior_opt', 'noise_amp', 0.05],\n",
    "            ['back_forth', 'n_back_forth', 5],\n",
    "            ['back_forth', 'scheduler_step_size', 1],\n",
    "            ['back_forth', 'scheduler_gamma', 0.5],\n",
    "            ['back_forth', 'first_func_in_fit', first_func]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        trainer_anomaly = Trainer(\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            dataset=ds_to_use.clone(),\n",
    "            attr_opt=True,\n",
    "            task='anomaly',\n",
    "            mighty_configs_dict=True,\n",
    "            config_triplets_to_change=config_triplets\n",
    "        )\n",
    "\n",
    "        losses, acc_test, acc_val = trainer_anomaly.train(\n",
    "            init_type='small_gaus',\n",
    "            init_feats=True,\n",
    "            acc_every=20,\n",
    "            plot_every=-1,\n",
    "            verbose=True,\n",
    "            verbose_in_funcs=False\n",
    "        )\n",
    "        losseses.append(losses)\n",
    "        acc_testses.append(acc_test)\n",
    "        acc_valses.append(acc_val)\n",
    "\n",
    "del ds, fat_ds\n",
    "\n",
    "#todo: test ieclam test distance. copy to another folder and remove all of the val stuff from anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracies(acc_testses[0], n_iter_1st=trainer_anomaly.configs_dict['feat_opt']['n_iter'], n_iter_2nd=trainer_anomaly.configs_dict['prior_opt']['n_iter'], n_back_forth=trainer_anomaly.configs_dict['back_forth']['n_back_forth'])\n",
    "#todo: test ieclam and bigclam and test distance.\n",
    "\n",
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: now the most important job is to put all of the datasets on the server and do cross validation to see the number of communitieis. so number of communitiies then mpnn on the vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Densification Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- COMPARE LOSSES WITH WITHOUT DENSIFICATION  -----\n",
    "#? REDDIT\n",
    "config_triplets = [\n",
    "    ['feat_opt', 'n_iter', 20000],\n",
    "    ['feat_opt', 'lr', 0.000001],\n",
    "    ]\n",
    "\n",
    "slim_reddit_iegam_losses, slim_reddit_iegam_anomaly_auc, slim_reddit_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'redditGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_reddit_iegam_losses, fat_reddit_iegam_anomaly_auc, fat_reddit_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'redditGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? PHOTO\n",
    "slim_photo_iegam_losses, slim_photo_iegam_anomaly_auc, slim_photo_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'photoGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_photo_iegam_losses, fat_photo_iegam_anomaly_auc, fat_photo_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'photoGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? ELLIPTIC\n",
    "slim_elliptic_iegam_losses, slim_elliptic_iegam_anomaly_auc, slim_elliptic_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ellipticGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_elliptic_losses, fat_elliptic_iegam_anomaly_auc, fat_elliptic_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ellipticGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? BlogCatalog\n",
    "slim_blog_iegam_losses, slim_blog_iegam_anomaly_auc, slim_blog_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'BlogCatalog', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_blog_iegam_losses, fat_blog_iegam_anomaly_auc, fat_blog_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'BlogCatalog', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "#? Flickr\n",
    "slim_flickr_iegam_losses, slim_flickr_iegam_anomaly_auc, slim_flickr_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'Flickr', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_flickr_iegam_losses, fat_flickr_iegam_anomaly_auc, fat_flickr_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'Flickr', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? ACM\n",
    "slim_acm_iegam_losses, slim_acm_iegam_anomaly_auc, slim_acm_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ACM', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_acm_iegam_losses, fat_acm_iegam_anomaly_auc, fat_acm_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ACM', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "#todo: make the percentage of edges to omit a function of the average DEGREE or something. not the number of edges because a graph can have many nodes and many edges and little nodes and many edges...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save everything!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#todo:\n",
    "#0. right now: doing the link prediction on the server\n",
    "#1. plot the loss compared to anomaly accuracy plot the link prediction compared to \n",
    "#2. read the output os the cheating to see if there is a pattern if not, try 100 feats 1000 prior and try to find some number for n_back_forth\n",
    "\n",
    "\n",
    "# we want to plot loss compared to link accuracy. find them from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "fat_reddit_auc = np.array(fat_reddit_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_reddit_auc = (np.repeat(fat_reddit_auc, 10) - np.mean(fat_reddit_auc) )* 1000000000\n",
    "slim_reddit_auc = np.array(slim_reddit_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_reddit_auc = (np.repeat(slim_reddit_auc, 10) - np.mean(slim_reddit_auc)) * 1000000000\n",
    "    \n",
    "fat_photo_auc = np.array(fat_photo_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_photo_auc = (np.repeat(fat_photo_auc, 10) - np.mean(fat_photo_auc) )* 1000000000\n",
    "slim_photo_auc = np.array(slim_photo_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_photo_auc = (np.repeat(slim_photo_auc, 10) - np.mean(slim_photo_auc)) * 1000000000\n",
    "\n",
    "fat_elliptic_auc = np.array(fat_elliptic_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_elliptic_auc = (np.repeat(fat_elliptic_auc, 10) - np.mean(fat_elliptic_auc)) * 1000000000\n",
    "slim_elliptic_auc = np.array(slim_elliptic_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_elliptic_auc = (np.repeat(slim_elliptic_auc, 10) - np.mean(slim_elliptic_auc)) * 1000000000\n",
    "\n",
    "fat_blog_auc = np.array(fat_blog_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_blog_auc = (np.repeat(fat_blog_auc, 10) - np.mean(fat_blog_auc)) * 1000000000\n",
    "slim_blog_auc = np.array(slim_blog_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_blog_auc = (np.repeat(slim_blog_auc, 10) - np.mean(slim_blog_auc)) * 1000000000\n",
    "\n",
    "fat_flickr_auc = np.array(fat_flickr_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_flickr_auc = (np.repeat(fat_flickr_auc, 10) - np.mean(fat_flickr_auc)) * 1000000000\n",
    "slim_flickr_auc = np.array(slim_flickr_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_flickr_auc = (np.repeat(slim_flickr_auc, 10) - np.mean(slim_flickr_auc)) * 1000000000\n",
    "    \n",
    "fat_acm_auc = np.array(fat_acm_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_acm_auc = (np.repeat(fat_acm_auc, 10) - np.mean(fat_acm_auc)) * 1000000000\n",
    "slim_acm_auc = np.array(slim_acm_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_acm_auc = (np.repeat(slim_acm_auc, 10) - np.mean(slim_acm_auc)) * 1000000000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(fat_flickr_iegam_anomaly_auc['vanilla_star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_shit(iter, losses, aucs):\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_reddit_iegam_losses[iter:]), label='loss_slim_reddit_iegam')\n",
    "        plt.plot(50*np.array(fat_reddit_iegam_losses[iter:]), label='loss_fat_reddit_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_reddit_auc[iter:], label='expanded_fat_reddit_auc')\n",
    "        plt.plot(expanded_slim_reddit_auc[iter:], label='expanded_slim_reddit_auc')\n",
    "    plt.legend()\n",
    "    plt.title('reddit')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_photo_iegam_losses[iter:]), label='loss_slim_photo_iegam')\n",
    "        plt.plot(50*np.array(fat_photo_iegam_losses[iter:]), label='loss_fat_photo_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_photo_auc[iter:], label='expanded_fat_photo_auc')\n",
    "        plt.plot(expanded_slim_photo_auc[iter:], label='expanded_slim_photo_auc')\n",
    "    plt.legend()\n",
    "    plt.title('photo')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_elliptic_iegam_losses[iter:]), label='loss_slim_elliptic_iegam')\n",
    "        plt.plot(50*np.array(fat_elliptic_losses[iter:]), label='loss_fat_elliptic_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_elliptic_auc[iter:], label='expanded_fat_elliptic_auc')\n",
    "        plt.plot(expanded_slim_elliptic_auc[iter:], label='expanded_slim_elliptic_auc')\n",
    "    plt.legend()\n",
    "    plt.title('elliptic')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_blog_iegam_losses[iter:]), label='loss_slim_blog_iegam')\n",
    "        plt.plot(50*np.array(fat_blog_iegam_losses[iter:]), label='loss_fat_blog_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_blog_auc[iter:], label='expanded_fat_blog_auc')\n",
    "        plt.plot(expanded_slim_blog_auc[iter:], label='expanded_slim_blog_auc')\n",
    "    plt.legend()\n",
    "    plt.title('blogcatalog')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_flickr_iegam_losses[iter:]), label='loss_slim_flickr_iegam')\n",
    "        plt.plot(50*np.array(fat_flickr_iegam_losses[iter:]), label='loss_fat_flickr_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_flickr_auc[iter:], label='expanded_fat_flickr_auc')\n",
    "        plt.plot(expanded_slim_flickr_auc[iter:], label='expanded_slim_flickr_auc')\n",
    "    plt.legend()\n",
    "    plt.title('flickr')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_acm_iegam_losses[iter:]), label='loss_slim_acm_iegam')\n",
    "        plt.plot(50*np.array(fat_acm_iegam_losses[iter:]), label='loss_fat_acm_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_acm_auc[iter:], label='expanded_fat_acm_auc')\n",
    "        plt.plot(expanded_slim_acm_auc[iter:], label='expanded_slim_acm_auc')\n",
    "    plt.legend()\n",
    "    plt.title('acm')\n",
    "\n",
    "# when it finished i want to see if the peak of the link vals \n",
    "# should remove more edges\n",
    "\n",
    "\n",
    "# if it improves losses then use it if not, don't.... something you can check for every ds\n",
    "\n",
    "plot_shit(iter=200,aucs=True, losses=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the i want to see when was the average early stop happening in prior and fit feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting link prediction for early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_iegam_flickr, label='iegam_flickr')\n",
    "plt.plot(_iegam_flickr, label='iegam_amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = []\n",
    "config_triplets_prior = []\n",
    "ad.anomaly_detection_unsupervised_priored(\n",
    "        dataset_name='redditGGAD',\n",
    "        model_name='iegam',\n",
    "        attr_opt=True,\n",
    "        config_triplets_prior=config_triplets_prior,\n",
    "        config_triplets_gam=config_triplets_gam,\n",
    "        external_ds=None,\n",
    "        use_fat=True,\n",
    "        optimize_vanilla=True,\n",
    "        optimize_prior=False,\n",
    "        performance_metric='best'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early stop anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Anomalies\n",
    "Created artificially in DOMINANT paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlogCatalog\n",
    "\n",
    "\n",
    "now the prior works, but before what worked and gave better results was the prior star. the method as i remember is similar- 10 iter feats and 1000 prior. maybe need to play some more with the lr...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA --\n",
    "blogcatalog_ds_with_anomalies = import_dataset('BlogCatalog')\n",
    "\n",
    "fat_blogcatalog_ds_with_anomalies = TwoHop()(blogcatalog_ds_with_anomalies)\n",
    "fat_blogcatalog_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_blogcatalog_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=blogcatalog_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    verbose=False)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=False)\n",
    "auc_gam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: test the greedy anomaly detection algorithm!\n",
    "\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "config_triplets_piegam = [\n",
    "                    \n",
    "                    ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    ['prior_opt', 'noise_amp', 0.05],\n",
    "                    ['feat_opt' , 'n_iter', 10000], # was good for blog catalog\n",
    "                    ['feat_opt', 'lr', 0.00001], \n",
    "                    # ['feat_opt', 'lr', 0.00000001], # good for when starting with trained piegam\n",
    "                    ['back_forth','n_back_forth', 100],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_feat', 36], # best so far 0.807 20 dim feat. 36 gave 0.85\n",
    "                    ['clamiter_init', 'dim_attr', 250] # 250 was the best 0.81\n",
    "                    ] \n",
    "\n",
    "config_triplets_pclam = [\n",
    "                    ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    ['prior_opt', 'noise_amp', 0.01],\n",
    "                    ['feat_opt' , 'n_iter', 10000], # was good for blog catalog\n",
    "                    ['feat_opt', 'lr', 0.00001], \n",
    "                    # ['feat_opt', 'lr', 0.00000001], # good for when starting with trained piegam\n",
    "                    ['back_forth','n_back_forth', 100],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_feat', 30],\n",
    "                    ['clamiter_init', 'dim_attr', 250]\n",
    "                    ] \n",
    "# use fat gave 0.788 0.79 pclam and 0.782 piegam. no fat gives over 80\n",
    "# fit feats in piegan gave 0.806, recent and best is 0.8109\n",
    "# fat good with prior\n",
    "ds_name = 'BlogCatalog'\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=False,\n",
    "                                        first_func_name='fit_feats',\n",
    "                                        performance_metric='prior_star',\n",
    "                                        early_stop_fit=4,\n",
    "                                        early_stops=[500,500],\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        first_func_name='fit_feats',\n",
    "                                        performance_metric='prior_star',\n",
    "                                        early_stop_fit=4,\n",
    "                                        early_stops=[500,500],\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "\n",
    "\n",
    "#TODO: right now finding attr dim, it's 250, 150 worked well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 20], # try even less, this is the lowest value i've tried\n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                     ] # best so far\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                    # ['back_forth', 'n_back_forth', 100],\n",
    "                    # ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "\n",
    "\n",
    "\n",
    "ds_name = 'Flickr'\n",
    "\n",
    "# best: 0.83 with prior\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "# 0.775 with prior stsar\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "\n",
    "#todo:  attr opt -> clamiter config (prior opt?)\n",
    "#todo:  optimize_vanilla -> back_forth      \n",
    "#todo:  first_func_name -> back_forth\n",
    "#todo:  early stop fit -> back_forth\n",
    "#todo:  early_stop_feats/prior->feat/prior_opt       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "\n",
    "# config_triplets_piegam = [\n",
    "#                     # ['back_forth', 'n_back_forth', 400],\n",
    "#                      # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "#                     ['prior_opt', 'n_iter', 2000],\n",
    "#                     ['prior_opt', 'noise_amp', 0.1],\n",
    "#                     # ['feat_opt' , 'n_iter', 10],\n",
    "#                     # ['feat_opt', 'lr', 0.00001],\n",
    "#                     # ['clamiter_init', 'dim_attr', 80],\n",
    "#                     # ['clamiter_init', 'dim_feat', 24],\n",
    "#                     ['clamiter_init', 'hidden_dim', 32],\n",
    "#                     ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "#                     ['clamiter_init', 'num_layers_mlp', 4]\n",
    "#                     ]\n",
    "\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                        #     ['back_forth', 'n_back_forth', 100],\n",
    "                        #   ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.01],\n",
    "                    # ['feat_opt' , 'n_iter', 500],\n",
    "                    # ['feat_opt', 'lr', 0.000001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 34],\n",
    "                    # ['clamiter_init', 'dim_attr',100]\n",
    "                     ] # best so far\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 500],\n",
    "                    # ['feat_opt', 'lr', 0.000001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 25],\n",
    "                    # ['clamiter_init', 'dim_attr',200]\n",
    "                     ] # best so far\n",
    "\n",
    "ds_name = 'ACM'\n",
    "\n",
    "# 0.842%\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='prior_star',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "# 0.849%\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "\n",
    "#todo: move things to the state dict \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGAD datasets anomaly earlystop\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#todo:  1.    make the ggad optimization work\n",
    "# todo: 2.    run with the correct results from the feature optimization\n",
    "# todo  3.    print intermediate values in feat_fit when run individually\n",
    "# todo  4.    move fit functions to trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elliptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#todo; add schefuling for fit feats in vanilla!\n",
    "config_triplets_piegam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #   ['back_forth','scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 10],\n",
    "                    # ['clamiter_init', 'dim_attr',70]] # best so far\n",
    "                    ]\n",
    "config_triplets_pclam = [\n",
    "    # `                   ['back_forth', 'n_back_forth', 100],\n",
    "    #                      ['back_forth','scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 10],\n",
    "                    # ['clamiter_init', 'dim_attr',70]] # best so far\n",
    "                    ]\n",
    "# best score here 0.69, sometimes 0.66 with prior. fat\n",
    "\n",
    "ds_name = 'ellipticGGAD'\n",
    "#! why does prior go up but prior star doesn't?\n",
    "\n",
    "#todo: try use_fat, best or the one that is best, with optimize vanilla and without with first func feats/prior\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        attr_opt=True, #! clamiter\n",
    "                                        optimize_vanilla=False, #! anomaly config\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=False,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "#todo: if there is an error print it and return the best value. make sure it's not inf or any of that nonsense...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                            # ['back_forth', 'n_back_forth', 40],\n",
    "                            # ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 40], \n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                    ] # best so far\n",
    "\n",
    "\n",
    "#got 58 with bigclam\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 40],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 20], # try even less, this is the lowest value i've tried\n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                    ] # best so far\n",
    "# PHOTO\n",
    "ds_name = 'photoGGAD'\n",
    "# raw attr for photoGGAD is 745 dimension\n",
    "# 0.60 is good here\n",
    "#todo: try use_fat, best or the one that is best, with optimize vanilla and without with first func feats/prior\n",
    "#PCALM does very good!! for some reason, on this dataset, prior improves but prior star doesn't change... vanilla doesn't change also...\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None, \n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: test the greedy anomaly detection algorithm!\n",
    "#************* SCHEDULER BABY YAHHHH *************** and remove the limitations from\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "config_triplets_piegam = [\n",
    "                    # ['back_forth', 'n_back_forth', 40],\n",
    "                    # ['back_forth', 'scheduler_step_size', 3],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                    # ['back_forth', 'n_back_forth', 40],\n",
    "                    # ['back_forth', 'scheduler_step_size', 3],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "# extra config for successful piegam, also there are no bounds for best auc.\n",
    "\n",
    "\n",
    "ds_name = 'redditGGAD'\n",
    "#todo: i want to have the reddit dataset without optimi\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best', \n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='prior_star',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first optimize vanilla so that no need to optimize it over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA -- (before the newest optimization)\n",
    "\n",
    "reddit_ds_with_anomalies = import_dataset('redditGGAD')\n",
    "\n",
    "fat_reddit_ds_with_anomalies = TwoHop()(reddit_ds_with_anomalies)\n",
    "fat_reddit_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_reddit_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    early_stop_fit=50,\n",
    "    verbose=True)\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfFINANCE \n",
    "i think i can do without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA --\n",
    "\n",
    "tfFinance_ds_with_anomalies = import_dataset('tfFinanceGGAD')\n",
    "\n",
    "fat_tfFinance_ds_with_anomalies = TwoHop()(tfFinance_ds_with_anomalies)\n",
    "fat_tfFinance_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_tfFinance_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=tfFinance_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    verbose=True)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['prior_opt', 'noise_amp', 0.1],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_attr', 10]] # best so far\n",
    "ds_name = 'tfFinanceGGAD'\n",
    "ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam', \n",
    "                                        external_ds=None,\n",
    "                                        attr_opt=True,config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_p,\n",
    "                                        use_fat=False, \n",
    "                                        optimize_vanilla=False)\n",
    "\n",
    "ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam', \n",
    "                                        external_ds=None,\n",
    "                                        attr_opt=True,config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_p, \n",
    "                                        use_fat=False,\n",
    "                                        optimize_vanilla=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUP PIEGAM BLOGCATALOG\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'dim_attr', 100]] # best so far\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='pclam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    inflation_flow_name=None,\n",
    "    attr_opt=True,\n",
    "    \n",
    "    device=device)\n",
    "#! problem with prior need to make the weights bigger.need to make this into a feature because it depends on the prior weights\n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_feats',\n",
    "            auc_every=1,\n",
    "            early_stop=3,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#? maybe stop the feat opt after a crtain point?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=blogcatalog_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[7, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=blogcatalog_ds, model_name='iegam', \n",
    "    ks=[6, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFICATION\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_blogcatalog_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[7, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=True)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_blogcatalog_ds, model_name='iegam', \n",
    "    ks=[6, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=True)\n",
    "\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n",
    "#todo: 2 communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "acm_ds = import_dataset('ACM', verbose=True)\n",
    "\n",
    "fat_acm_ds = TwoHop()(acm_ds)\n",
    "fat_acm_ds.edge_attr = torch.ones(\n",
    "                fat_acm_ds.edge_attr.shape[0])\n",
    "acm_ds.attr\n",
    "#? doing mask to index and index to mask to remove isolated nodes in dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised acm\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam_acm = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=acm_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam_acm.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 200],\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],]\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam_acm, trainer_gam_acm.data, verbose=True)\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam_acm.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "losses_piegam = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "\n",
    "plot_losses(losses_piegam[0], losses_piegam[1])\n",
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "# could it be that i removed nodes and then the gt nomalous changed?\n",
    "\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=acm_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=acm_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "# can we know which anomalies are feature based and which topological?\n",
    "# todo: check if the labeling changes when you remove nodes.\n",
    "blogcatalog_ds = import_dataset('BlogCatalog', attr_transform='truncated_svd', n_components=100)\n",
    "\n",
    "fat_blogcatalog_ds = TwoHop()(blogcatalog_ds)\n",
    "fat_blogcatalog_ds.edge_attr = torch.ones(\n",
    "                fat_blogcatalog_ds.edge_attr.shape[0])\n",
    "\n",
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogcatalog_ds.attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised blogcatalog\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=blogcatalog_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "\n",
    "auc_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "\n",
    "#todo: can also try to first fit iegam no prior and then do the 100 back forth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUP PIEGAM BLOGCATALOG\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'dim_attr', 100]] # best so far\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    inflation_flow_name=None,\n",
    "    attr_opt=True,\n",
    "    \n",
    "    device=device)\n",
    "#! problem with prior need to make the weights bigger.need to make this into a feature because it depends on the prior weights\n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_feats',\n",
    "            auc_every=1,\n",
    "            early_stop=3,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#? maybe stop the feat opt after a crtain point?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFICATION\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_acm_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_acm_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "\n",
    "# todo things to try: partial densification\n",
    "flickr_ds = import_dataset('Flickr')\n",
    "\n",
    "fat_flickr_ds = TwoHop()(flickr_ds)\n",
    "fat_flickr_ds.edge_attr = torch.ones(\n",
    "                fat_flickr_ds.edge_attr.shape[0])\n",
    "\n",
    "flickr_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised acm\n",
    "#todo: make this into a function! \n",
    "#todo: make function\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=flickr_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 200],\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],]\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "losses_piegam = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "\n",
    "plot_losses(losses_piegam[0], losses_piegam[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "# make a test for the server to use prior or stuff... don't know shit. prior unsupervised\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=flickr_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=flickr_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFIED\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_flickr_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_flickr_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n",
    "#! maybe edge attr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGAD datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigclam: 18 communities, 1700 iters\n",
    "iegam: 24 communities, 2000 iters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reddit_ds_with_anomalies = import_dataset('redditGGAD')\n",
    "\n",
    "fat_reddit_ds_with_anomalies = TwoHop()(reddit_ds_with_anomalies)\n",
    "fat_reddit_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_reddit_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "# reddit_ds_with_anomalies.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED bigclam\n",
    "# ============\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 1700]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_triplets_p = [\n",
    "                     ['feat_opt', 'n_iter', 10],\n",
    "                     ['prior_opt', 'n_iter', 1000],\n",
    "                     ['back_forth', 'n_back_forth', 100],\n",
    "                     ]\n",
    "trainer_piegam = Trainer( \n",
    "            model_name='pclam', \n",
    "            dataset=trainer_clam.data.clone(),\n",
    "            config_triplets_to_change=config_triplets_p,\n",
    "            inflation_flow_name=None,\n",
    "            attr_opt=True,\n",
    "            device=device)\n",
    "        \n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            auc_every=1,\n",
    "            early_stop=2,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "        \n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#TODO: for this situation, need to make the early stop checks at the beginning so it doesn't get ruined by the back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_vanilla = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='vanilla_star')\n",
    "\n",
    "auc_prior = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='prior')\n",
    "\n",
    "auc_prior_star = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='prior_star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "# gam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGCLAM UNSUPERVISED\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 1700]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "auc_bigclam, ap_bigclam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, verbose=True)\n",
    "\n",
    "# UNSUPERVISED clam\n",
    "# ============\n",
    "#todo: congrads! prior improves on unsupervised! there is some problem with star probs being infinite... take care of it.\n",
    "#* bigclam auc was 0.57 and improved with prior to 0.62\n",
    "config_triplets_clam = []\n",
    "# -- Train Base ---\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "losses_bigclam = trainer_clam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='small_gaus')[0]\n",
    "plot_losses(losses_bigclam) # GPU one round added 10000 mib...\n",
    "auc_bigclam, ap_bigclam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, verbose=True)\n",
    "\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_pclam = Trainer( \n",
    "    model_name='pclam', \n",
    "    dataset=trainer_clam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_pclam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n",
    "\n",
    "#! make unsupervised and semi supervised option for the trainer?\n",
    "#! try special densification?\n",
    "# options on how to make semi supervised:\n",
    "# 1. train all nodes then teach prior on the train labeled set\n",
    "# 2. densify in the special way the train labeled set \n",
    "# do just unsupervised piegam: train the prior on everything\n",
    "# then do semi supervised in two versions: densify everything train prior then train bigclam\n",
    "#todo: need to put a mask. maybe divide in the anomaly detector into supervised and semi supervised...\n",
    "#todo: question: if i initialize a trainer with a dataset that has features what happens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elliptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_ds = import_dataset('ellipticGGAD')\n",
    "\n",
    "fat_elliptic_ds = TwoHop()(elliptic_ds)\n",
    "fat_elliptic_ds.edge_attr = torch.ones(\n",
    "                fat_elliptic_ds.edge_attr.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_elliptic_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True, ret_ap=True)\n",
    "# gam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_ds = import_dataset('photoGGAD')\n",
    "\n",
    "fat_photo_ds = TwoHop()(photo_ds)\n",
    "fat_photo_ds.edge_attr = torch.ones(\n",
    "                fat_photo_ds.edge_attr.shape[0])\n",
    "\n",
    "photo_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_photo_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfFinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfFinance_ds = import_dataset('photoGGAD')\n",
    "\n",
    "fat_tfFinance_ds = TwoHop()(tfFinance_ds)\n",
    "fat_tfFinance_ds.edge_attr = torch.ones(\n",
    "                fat_tfFinance_ds.edge_attr.shape[0])\n",
    "\n",
    "tfFinance_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=tfFinance_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_ds = import_dataset('amazonGGAD')\n",
    "\n",
    "fat_amazon_ds = TwoHop()(amazon_ds)\n",
    "fat_amazon_ds.edge_attr = torch.ones(\n",
    "                fat_amazon_ds.edge_attr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "#! no memory! check why\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_amazon_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED GAM\n",
    "# ============\n",
    "'''train prior on all nodes without second clam training (didn't work when i tried)'''\n",
    "config_triplets_gam = []\n",
    "# -- Train Base ---\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "losses_iegam = trainer_gam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='small_gaus')[0]\n",
    "plot_losses(losses_iegam) # GPU one round added 10000 mib...\n",
    "auc_iegam, ap_iegam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Train With Prior --\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n",
    "\n",
    "#! make unsupervised and semi supervised option for the trainer?\n",
    "#! try special densification?\n",
    "# options on how to make semi supervised:\n",
    "# 1. train all nodes then teach prior on the train labeled set\n",
    "# 2. densify in the special way the train labeled set \n",
    "# do just unsupervised piegam: train the prior on everything\n",
    "# then do semi supervised in two versions: densify everything train prior then train bigclam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMAL NEIGH EXPERIMENT #! not very good....\n",
    "# ======================== #! takes a super long time and doesn't improve anything\n",
    "\n",
    "\n",
    "config_triplet = []\n",
    "\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies,\n",
    "    config_triplets_to_change=config_triplet,\n",
    "    device=device)\n",
    "\n",
    "losses_bigclam = trainer_clam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='minimal_neigh')[0]\n",
    "plot_losses(losses_bigclam) # GPU one round added 10000 mib...\n",
    "auc, ap = ad.classify_unsupervised(trainer_clam, fat_reddit_ds_with_anomalies, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to plot scatter 2000 vs 3000 to see if there is a diffrernce\n",
    "# seems like 2000 is better.... but can't relly know\n",
    "ks_aucs_ie_comb = np.concatenate([ks_aucs_aps_iegam_2000, ks_aucs_aps_iegam_3000], axis=1)\n",
    "ks_aucs_bc_comb = np.concatenate([ks_aucs_aps_bigclam_2000, ks_aucs_aps_bigclam_3000], axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_iegam_2000[0], ks_aucs_aps_iegam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_bigclam_2000[0], ks_aucs_aps_bigclam_2000[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_ie_comb[0], ks_aucs_ie_comb[1])\n",
    "plt.scatter(ks_aucs_bc_comb[0], ks_aucs_bc_comb[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_bigclam_2000[0], ks_aucs_aps_bigclam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_bigclam_3000[0], ks_aucs_aps_bigclam_3000[1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_iegam_2000[0], ks_aucs_aps_iegam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_iegam_3000[0], ks_aucs_aps_iegam_3000[1])\n",
    "\n",
    "\n",
    "\n",
    "#todo: save the 2000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_aucs_aps_iegam_2000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'iegam', [16, 16, 18,18, 20, 20, 22, 22, 24,24], 2000, device=device, return_ap=True)\n",
    "\n",
    "ks_aucs_aps_iegam_3000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'iegam', [16, 16, 18,18, 20, 20, 22, 22, 24,24], 3000, device=device, return_ap=True)\n",
    "\n",
    "\n",
    "\n",
    "ks_aucs_aps_bigclam_2000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'bigclam', [16, 16,17, 17, 18,18, 19,19,20,20,21,21,22,22] , 2000, device=device, return_ap=True)\n",
    "\n",
    "ks_aucs_aps_bigclam_3000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'bigclam', [16, 16,17, 17, 18,18, 19,19,20,20,21,21,22,22], 3000, device=device, return_ap=True)\n",
    "\n",
    "#todo: compare number of iterations and see if there is a significant difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_prior = ad.classify_test_set_no_trainer(fat_reddit_ds_with_anomalies, prior, ll_type='prior')[0]\n",
    "auc_prior_star = ad.classify_test_set_no_trainer(fat_reddit_ds_with_anomalies, prior, ll_type='prior_star')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_prediction_adj(trainer_bigclam.data, lorenz=False)\n",
    "plot_adj(fb_data_with_anomalies.edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and train unsupervised:\n",
    "# train bigclam and classify vanilla star\n",
    "# make the fat ds with anomalies with all of the train test labels and stuff just init all of the node features. maybe even just gaussian for everything. then initialize trainer with the dataset\n",
    "fb_data = import_dataset('facebook348')\n",
    "fb_data_with_anomalies = ad.add_anomalies(fb_data, add_method='uniform', avg_deg_factor=1)\n",
    "#densify the fb_data_with_anomalies\n",
    "fat_fb_data_with_anomalies = TwoHop()(fb_data_with_anomalies)\n",
    "\n",
    "trainer_bigclam = Trainer(\n",
    "    dataset_name='facebook348', \n",
    "    model_name='bigclam', \n",
    "    dataset=fat_fb_data_with_anomalies,\n",
    "    device=device)\n",
    "\n",
    "# now train the trainer and run vanilla classification to see if we get weird places for anomalies\n",
    "\n",
    "losses_bigclam = trainer_bigclam.train_model_on_params()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc, ll_normal,ll_anomalies = ad.classify_unsupervised(\n",
    "    trainer_bigclam, \n",
    "    trainer_bigclam.data,\n",
    "    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piegam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
