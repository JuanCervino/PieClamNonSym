{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load the model from the file \"sbm3x3_pclam_roc_0.210_auc_0.860\"\n",
    "import torch\n",
    "from torch_geometric.utils import dropout_node, contains_isolated_nodes, is_undirected, contains_self_loops, is_undirected, to_undirected, k_hop_subgraph, coalesce, subgraph, to_dense_adj\n",
    "from torch.autograd import grad\n",
    "from torch_geometric.transforms import TwoHop\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from math import floor\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "from transformation import RealNVP\n",
    "import transformation as tr\n",
    "from datasets.import_dataset import import_dataset, load_data_GGAD\n",
    "import clamiter as ci\n",
    "from utils import utils\n",
    "from utils.plotting import *\n",
    "from trainer import Trainer\n",
    "from tests import test_no_duplicacy, test_get_fat_ds_test, test_densify_test_from_train\n",
    "from scripts.scripting_utils import print_prior_training_stats\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/danny/AAAI_pieclam/tests/../datasets/import_dataset.py:410: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  edge_index = torch.tensor(adj.nonzero(), dtype=torch.long)\n",
      "/home/user/anaconda3/envs/piegam/lib/python3.11/site-packages/torch_geometric/edge_index.py:784: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:415:::  \n",
      " starting optimization of piegam on ellipticGGAD on device cuda\n",
      "\n",
      " configs_dict: \n",
      "{\n",
      "    \"clamiter_init\": {\n",
      "        \"dim_feat\": 30,\n",
      "        \"dim_attr\": 93,\n",
      "        \"s_reg\": 0.0,\n",
      "        \"l1_reg\": 1,\n",
      "        \"T\": 1,\n",
      "        \"hidden_dim\": 64,\n",
      "        \"num_coupling_blocks\": 32,\n",
      "        \"num_layers_mlp\": 2\n",
      "    },\n",
      "    \"feat_opt\": {\n",
      "        \"lr\": 3e-06,\n",
      "        \"n_iter\": 500,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"prior_opt\": {\n",
      "        \"n_iter\": 1300,\n",
      "        \"lr\": 2e-06,\n",
      "        \"noise_amp\": 0.05,\n",
      "        \"weight_decay\": 0.1,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"back_forth\": {\n",
      "        \"n_back_forth\": 5,\n",
      "        \"scheduler_step_size\": 1,\n",
      "        \"scheduler_gamma\": 0.5,\n",
      "        \"early_stop_fit\": 0,\n",
      "        \"first_func_in_fit\": \"fit_feats\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:420:::  \n",
      " train_model_on_params, initializing feats with small_gaus\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:428:::  \n",
      " init_node_feats took 0.0039463043212890625 seconds\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:499:::  \n",
      "fit, task='anomaly'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:542:::  \n",
      "in fit,\n",
      "first_func_in_fit='fit_feats'\n",
      "second_function_name='fit_prior'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 1/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 1/5 took 97.90268039703369 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.43477285331747617 prior: 0.631854360648733 prior_star: 0.4490449072509102 \n",
      "Best:\n",
      "vanilla_star: 0.48943280048024634 at iteration 19\n",
      "prior: 0.631854360648733 at iteration 1799\n",
      "prior_star: 0.4881454961439948 at iteration 19\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 3e-06 to 1.5e-06\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.05 to 0.025\n",
      "prior lr from 2e-06 to 1e-06\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 2/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 2/5 took 99.2728488445282 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.4340386003398088 prior: 0.5850355950623073 prior_star: 0.4953492695451548 \n",
      "Best:\n",
      "vanilla_star: 0.48943280048024634 at iteration 19\n",
      "prior: 0.6448406482019214 at iteration 2579\n",
      "prior_star: 0.4953492695451548 at iteration 3599\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 1.5e-06 to 7.5e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.025 to 0.0125\n",
      "prior lr from 1e-06 to 5e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 3/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 3/5 took 98.4123284816742 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.4340642029506361 prior: 0.5795999142221508 prior_star: 0.511471897309839 \n",
      "Best:\n",
      "vanilla_star: 0.48943280048024634 at iteration 19\n",
      "prior: 0.6448406482019214 at iteration 2579\n",
      "prior_star: 0.511471897309839 at iteration 5399\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 7.5e-07 to 3.75e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.0125 to 0.00625\n",
      "prior lr from 5e-07 to 2.5e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 4/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 4/5 took 100.51354002952576 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.4337568450243282 prior: 0.5782553883219783 prior_star: 0.5179334428756139 \n",
      "Best:\n",
      "vanilla_star: 0.48943280048024634 at iteration 19\n",
      "prior: 0.6448406482019214 at iteration 2579\n",
      "prior_star: 0.5179334428756139 at iteration 7199\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 3.75e-07 to 1.875e-07\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.00625 to 0.003125\n",
      "prior lr from 2.5e-07 to 1.25e-07\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:600:::  \n",
      "back and forth 5/5\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:643:::  \n",
      "fit, back and forth 5/5 took 98.8573956489563 seconds\n",
      "ANOMALY TEST accuracy.\n",
      "Latest:\n",
      "vanilla_star: 0.43335642670166025 prior: 0.5765111011946359 prior_star: 0.5200928214489475 \n",
      "Best:\n",
      "vanilla_star: 0.48943280048024634 at iteration 19\n",
      "prior: 0.6448406482019214 at iteration 2579\n",
      "prior_star: 0.5200928214489475 at iteration 8999\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../utils/utils.py:746:::  \n",
      "scheduler made step. changes:\n",
      "feats lr: 1.875e-07 to 9.375e-08\n",
      "feats n_iter changed from 500 to 500\n",
      "noise_amp from 0.003125 to 0.0015625\n",
      "prior lr from 1.25e-07 to 6.25e-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../clamiter.py:663:::  \n",
      "fit end, no early stopping\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:502:::  \n",
      "train_model_on_params on piegam ellipticGGAD \n",
      "took 495.16659593582153 seconds\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/tests/../trainer.py:504:::  \n",
      "\n",
      "\n",
      "FINISHED train model on params \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'piegam'\n",
    "ds_name = 'ellipticGGAD'\n",
    "#! problem with something to do with attr with reddit and elliptic\n",
    "ds = import_dataset(ds_name)\n",
    "fat_ds = TwoHop()(ds)\n",
    "fat_ds.edge_attr = torch.ones(fat_ds.edge_index.shape[1]).bool()\n",
    "\n",
    "if ds_name in ['Flickr', 'ACM', 'BlogCatalog']:\n",
    "    ds_to_use = ds\n",
    "elif ds_name in ['redditGGAD', 'photoGGAD', 'ellipticGGAD']:\n",
    "    ds_to_use = fat_ds\n",
    "else:\n",
    "    raise ValueError('ds_name not recognized')\n",
    "\n",
    "\n",
    "losseses = []\n",
    "acc_testses = []\n",
    "acc_valses = []\n",
    "for first_func in ['fit_feats']:\n",
    "    for model_name in ['piegam']:\n",
    "        config_triplets = [\n",
    "            ['clamiter_init', 'dim_feat', 30],\n",
    "            ['clamiter_init', 'dim_attr', 100],\n",
    "            # ['feat_opt','n_iter', 100],\n",
    "            # ['prior_opt', 'n_iter', 200],\n",
    "            ['feat_opt', 'lr', 3e-6],\n",
    "            ['prior_opt', 'lr', 2e-6],\n",
    "            ['prior_opt', 'noise_amp', 0.05],\n",
    "            ['back_forth', 'n_back_forth', 5],\n",
    "            ['back_forth', 'scheduler_step_size', 1],\n",
    "            ['back_forth', 'scheduler_gamma', 0.5],\n",
    "            ['back_forth', 'first_func_in_fit', first_func]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        trainer_anomaly = Trainer(\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            dataset=ds_to_use.clone(),\n",
    "            attr_opt=True,\n",
    "            task='anomaly',\n",
    "            mighty_configs_dict=True,\n",
    "            config_triplets_to_change=config_triplets\n",
    "        )\n",
    "\n",
    "        losses, acc_test, acc_val = trainer_anomaly.train(\n",
    "            init_type='small_gaus',\n",
    "            init_feats=True,\n",
    "            acc_every=20,\n",
    "            plot_every=-1,\n",
    "            verbose=True,\n",
    "            verbose_in_funcs=False\n",
    "        )\n",
    "        losseses.append(losses)\n",
    "        acc_testses.append(acc_test)\n",
    "        acc_valses.append(acc_val)\n",
    "\n",
    "del ds, fat_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_accuracies(acc_testses[0], n_iter_1st=trainer_anomaly.configs_dict['feat_opt']['n_iter'], n_iter_2nd=trainer_anomaly.configs_dict['prior_opt']['n_iter'], n_back_forth=trainer_anomaly.configs_dict['back_forth']['n_back_forth'])\n",
    "#todo: test ieclam and bigclam and test distance.\n",
    "\n",
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: now the most important job is to put all of the datasets on the server and do cross validation to see the number of communitieis. so number of communitiies then mpnn on the vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Densification Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- COMPARE LOSSES WITH WITHOUT DENSIFICATION  -----\n",
    "#? REDDIT\n",
    "config_triplets = [\n",
    "    ['feat_opt', 'n_iter', 20000],\n",
    "    ['feat_opt', 'lr', 0.000001],\n",
    "    ]\n",
    "\n",
    "slim_reddit_iegam_losses, slim_reddit_iegam_anomaly_auc, slim_reddit_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'redditGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_reddit_iegam_losses, fat_reddit_iegam_anomaly_auc, fat_reddit_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'redditGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? PHOTO\n",
    "slim_photo_iegam_losses, slim_photo_iegam_anomaly_auc, slim_photo_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'photoGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_photo_iegam_losses, fat_photo_iegam_anomaly_auc, fat_photo_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'photoGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? ELLIPTIC\n",
    "slim_elliptic_iegam_losses, slim_elliptic_iegam_anomaly_auc, slim_elliptic_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ellipticGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_elliptic_losses, fat_elliptic_iegam_anomaly_auc, fat_elliptic_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ellipticGGAD', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? BlogCatalog\n",
    "slim_blog_iegam_losses, slim_blog_iegam_anomaly_auc, slim_blog_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'BlogCatalog', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_blog_iegam_losses, fat_blog_iegam_anomaly_auc, fat_blog_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'BlogCatalog', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "#? Flickr\n",
    "slim_flickr_iegam_losses, slim_flickr_iegam_anomaly_auc, slim_flickr_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'Flickr', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_flickr_iegam_losses, fat_flickr_iegam_anomaly_auc, fat_flickr_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'Flickr', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "#? ACM\n",
    "slim_acm_iegam_losses, slim_acm_iegam_anomaly_auc, slim_acm_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ACM', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=False,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "fat_acm_iegam_losses, fat_acm_iegam_anomaly_auc, fat_acm_iegam_link_auc = ad.classify_anomaly_link_earlystop(\n",
    "    'ACM', \n",
    "    'iegam', \n",
    "    config_triplets_clam=config_triplets,\n",
    "    use_fat=True,\n",
    "    percentage_of_dyads_to_omit=0,\n",
    "    device=device,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "#todo: make the percentage of edges to omit a function of the average DEGREE or something. not the number of edges because a graph can have many nodes and many edges and little nodes and many edges...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save everything!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#todo:\n",
    "#0. right now: doing the link prediction on the server\n",
    "#1. plot the loss compared to anomaly accuracy plot the link prediction compared to \n",
    "#2. read the output os the cheating to see if there is a pattern if not, try 100 feats 1000 prior and try to find some number for n_back_forth\n",
    "\n",
    "\n",
    "# we want to plot loss compared to link accuracy. find them from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "fat_reddit_auc = np.array(fat_reddit_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_reddit_auc = (np.repeat(fat_reddit_auc, 10) - np.mean(fat_reddit_auc) )* 1000000000\n",
    "slim_reddit_auc = np.array(slim_reddit_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_reddit_auc = (np.repeat(slim_reddit_auc, 10) - np.mean(slim_reddit_auc)) * 1000000000\n",
    "    \n",
    "fat_photo_auc = np.array(fat_photo_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_photo_auc = (np.repeat(fat_photo_auc, 10) - np.mean(fat_photo_auc) )* 1000000000\n",
    "slim_photo_auc = np.array(slim_photo_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_photo_auc = (np.repeat(slim_photo_auc, 10) - np.mean(slim_photo_auc)) * 1000000000\n",
    "\n",
    "fat_elliptic_auc = np.array(fat_elliptic_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_elliptic_auc = (np.repeat(fat_elliptic_auc, 10) - np.mean(fat_elliptic_auc)) * 1000000000\n",
    "slim_elliptic_auc = np.array(slim_elliptic_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_elliptic_auc = (np.repeat(slim_elliptic_auc, 10) - np.mean(slim_elliptic_auc)) * 1000000000\n",
    "\n",
    "fat_blog_auc = np.array(fat_blog_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_blog_auc = (np.repeat(fat_blog_auc, 10) - np.mean(fat_blog_auc)) * 1000000000\n",
    "slim_blog_auc = np.array(slim_blog_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_blog_auc = (np.repeat(slim_blog_auc, 10) - np.mean(slim_blog_auc)) * 1000000000\n",
    "\n",
    "fat_flickr_auc = np.array(fat_flickr_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_flickr_auc = (np.repeat(fat_flickr_auc, 10) - np.mean(fat_flickr_auc)) * 1000000000\n",
    "slim_flickr_auc = np.array(slim_flickr_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_flickr_auc = (np.repeat(slim_flickr_auc, 10) - np.mean(slim_flickr_auc)) * 1000000000\n",
    "    \n",
    "fat_acm_auc = np.array(fat_acm_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_fat_acm_auc = (np.repeat(fat_acm_auc, 10) - np.mean(fat_acm_auc)) * 1000000000\n",
    "slim_acm_auc = np.array(slim_acm_iegam_anomaly_auc['vanilla_star'])\n",
    "expanded_slim_acm_auc = (np.repeat(slim_acm_auc, 10) - np.mean(slim_acm_auc)) * 1000000000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(fat_flickr_iegam_anomaly_auc['vanilla_star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_shit(iter, losses, aucs):\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_reddit_iegam_losses[iter:]), label='loss_slim_reddit_iegam')\n",
    "        plt.plot(50*np.array(fat_reddit_iegam_losses[iter:]), label='loss_fat_reddit_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_reddit_auc[iter:], label='expanded_fat_reddit_auc')\n",
    "        plt.plot(expanded_slim_reddit_auc[iter:], label='expanded_slim_reddit_auc')\n",
    "    plt.legend()\n",
    "    plt.title('reddit')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_photo_iegam_losses[iter:]), label='loss_slim_photo_iegam')\n",
    "        plt.plot(50*np.array(fat_photo_iegam_losses[iter:]), label='loss_fat_photo_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_photo_auc[iter:], label='expanded_fat_photo_auc')\n",
    "        plt.plot(expanded_slim_photo_auc[iter:], label='expanded_slim_photo_auc')\n",
    "    plt.legend()\n",
    "    plt.title('photo')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_elliptic_iegam_losses[iter:]), label='loss_slim_elliptic_iegam')\n",
    "        plt.plot(50*np.array(fat_elliptic_losses[iter:]), label='loss_fat_elliptic_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_elliptic_auc[iter:], label='expanded_fat_elliptic_auc')\n",
    "        plt.plot(expanded_slim_elliptic_auc[iter:], label='expanded_slim_elliptic_auc')\n",
    "    plt.legend()\n",
    "    plt.title('elliptic')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_blog_iegam_losses[iter:]), label='loss_slim_blog_iegam')\n",
    "        plt.plot(50*np.array(fat_blog_iegam_losses[iter:]), label='loss_fat_blog_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_blog_auc[iter:], label='expanded_fat_blog_auc')\n",
    "        plt.plot(expanded_slim_blog_auc[iter:], label='expanded_slim_blog_auc')\n",
    "    plt.legend()\n",
    "    plt.title('blogcatalog')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_flickr_iegam_losses[iter:]), label='loss_slim_flickr_iegam')\n",
    "        plt.plot(50*np.array(fat_flickr_iegam_losses[iter:]), label='loss_fat_flickr_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_flickr_auc[iter:], label='expanded_fat_flickr_auc')\n",
    "        plt.plot(expanded_slim_flickr_auc[iter:], label='expanded_slim_flickr_auc')\n",
    "    plt.legend()\n",
    "    plt.title('flickr')\n",
    "\n",
    "    plt.figure()\n",
    "    if losses:\n",
    "        plt.plot(100*np.array(slim_acm_iegam_losses[iter:]), label='loss_slim_acm_iegam')\n",
    "        plt.plot(50*np.array(fat_acm_iegam_losses[iter:]), label='loss_fat_acm_iegam')\n",
    "    if aucs:\n",
    "        plt.plot(expanded_fat_acm_auc[iter:], label='expanded_fat_acm_auc')\n",
    "        plt.plot(expanded_slim_acm_auc[iter:], label='expanded_slim_acm_auc')\n",
    "    plt.legend()\n",
    "    plt.title('acm')\n",
    "\n",
    "# when it finished i want to see if the peak of the link vals \n",
    "# should remove more edges\n",
    "\n",
    "\n",
    "# if it improves losses then use it if not, don't.... something you can check for every ds\n",
    "\n",
    "plot_shit(iter=200,aucs=True, losses=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the i want to see when was the average early stop happening in prior and fit feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting link prediction for early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_iegam_flickr, label='iegam_flickr')\n",
    "plt.plot(_iegam_flickr, label='iegam_amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = []\n",
    "config_triplets_prior = []\n",
    "ad.anomaly_detection_unsupervised_priored(\n",
    "        dataset_name='redditGGAD',\n",
    "        model_name='iegam',\n",
    "        attr_opt=True,\n",
    "        config_triplets_prior=config_triplets_prior,\n",
    "        config_triplets_gam=config_triplets_gam,\n",
    "        external_ds=None,\n",
    "        use_fat=True,\n",
    "        optimize_vanilla=True,\n",
    "        optimize_prior=False,\n",
    "        performance_metric='best'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early stop anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Anomalies\n",
    "Created artificially in DOMINANT paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlogCatalog\n",
    "\n",
    "\n",
    "now the prior works, but before what worked and gave better results was the prior star. the method as i remember is similar- 10 iter feats and 1000 prior. maybe need to play some more with the lr...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA --\n",
    "blogcatalog_ds_with_anomalies = import_dataset('BlogCatalog')\n",
    "\n",
    "fat_blogcatalog_ds_with_anomalies = TwoHop()(blogcatalog_ds_with_anomalies)\n",
    "fat_blogcatalog_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_blogcatalog_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=blogcatalog_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    verbose=False)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=False)\n",
    "auc_gam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: test the greedy anomaly detection algorithm!\n",
    "\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "config_triplets_piegam = [\n",
    "                    \n",
    "                    ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    ['prior_opt', 'noise_amp', 0.05],\n",
    "                    ['feat_opt' , 'n_iter', 10000], # was good for blog catalog\n",
    "                    ['feat_opt', 'lr', 0.00001], \n",
    "                    # ['feat_opt', 'lr', 0.00000001], # good for when starting with trained piegam\n",
    "                    ['back_forth','n_back_forth', 100],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_feat', 36], # best so far 0.807 20 dim feat. 36 gave 0.85\n",
    "                    ['clamiter_init', 'dim_attr', 250] # 250 was the best 0.81\n",
    "                    ] \n",
    "\n",
    "config_triplets_pclam = [\n",
    "                    ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    ['prior_opt', 'noise_amp', 0.01],\n",
    "                    ['feat_opt' , 'n_iter', 10000], # was good for blog catalog\n",
    "                    ['feat_opt', 'lr', 0.00001], \n",
    "                    # ['feat_opt', 'lr', 0.00000001], # good for when starting with trained piegam\n",
    "                    ['back_forth','n_back_forth', 100],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_feat', 30],\n",
    "                    ['clamiter_init', 'dim_attr', 250]\n",
    "                    ] \n",
    "# use fat gave 0.788 0.79 pclam and 0.782 piegam. no fat gives over 80\n",
    "# fit feats in piegan gave 0.806, recent and best is 0.8109\n",
    "# fat good with prior\n",
    "ds_name = 'BlogCatalog'\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=False,\n",
    "                                        first_func_name='fit_feats',\n",
    "                                        performance_metric='prior_star',\n",
    "                                        early_stop_fit=4,\n",
    "                                        early_stops=[500,500],\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        first_func_name='fit_feats',\n",
    "                                        performance_metric='prior_star',\n",
    "                                        early_stop_fit=4,\n",
    "                                        early_stops=[500,500],\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "\n",
    "\n",
    "#TODO: right now finding attr dim, it's 250, 150 worked well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 20], # try even less, this is the lowest value i've tried\n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                     ] # best so far\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                    # ['back_forth', 'n_back_forth', 100],\n",
    "                    # ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "\n",
    "\n",
    "\n",
    "ds_name = 'Flickr'\n",
    "\n",
    "# best: 0.83 with prior\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "# 0.775 with prior stsar\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "\n",
    "#todo:  attr opt -> clamiter config (prior opt?)\n",
    "#todo:  optimize_vanilla -> back_forth      \n",
    "#todo:  first_func_name -> back_forth\n",
    "#todo:  early stop fit -> back_forth\n",
    "#todo:  early_stop_feats/prior->feat/prior_opt       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "\n",
    "# config_triplets_piegam = [\n",
    "#                     # ['back_forth', 'n_back_forth', 400],\n",
    "#                      # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "#                     ['prior_opt', 'n_iter', 2000],\n",
    "#                     ['prior_opt', 'noise_amp', 0.1],\n",
    "#                     # ['feat_opt' , 'n_iter', 10],\n",
    "#                     # ['feat_opt', 'lr', 0.00001],\n",
    "#                     # ['clamiter_init', 'dim_attr', 80],\n",
    "#                     # ['clamiter_init', 'dim_feat', 24],\n",
    "#                     ['clamiter_init', 'hidden_dim', 32],\n",
    "#                     ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "#                     ['clamiter_init', 'num_layers_mlp', 4]\n",
    "#                     ]\n",
    "\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                        #     ['back_forth', 'n_back_forth', 100],\n",
    "                        #   ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.01],\n",
    "                    # ['feat_opt' , 'n_iter', 500],\n",
    "                    # ['feat_opt', 'lr', 0.000001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 34],\n",
    "                    # ['clamiter_init', 'dim_attr',100]\n",
    "                     ] # best so far\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 500],\n",
    "                    # ['feat_opt', 'lr', 0.000001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 25],\n",
    "                    # ['clamiter_init', 'dim_attr',200]\n",
    "                     ] # best so far\n",
    "\n",
    "ds_name = 'ACM'\n",
    "\n",
    "# 0.842%\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='prior_star',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "# 0.849%\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "\n",
    "#todo: move things to the state dict \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGAD datasets anomaly earlystop\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#todo:  1.    make the ggad optimization work\n",
    "# todo: 2.    run with the correct results from the feature optimization\n",
    "# todo  3.    print intermediate values in feat_fit when run individually\n",
    "# todo  4.    move fit functions to trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elliptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#todo; add schefuling for fit feats in vanilla!\n",
    "config_triplets_piegam = [\n",
    "                        # ['back_forth', 'n_back_forth', 100],\n",
    "                        #   ['back_forth','scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 10],\n",
    "                    # ['clamiter_init', 'dim_attr',70]] # best so far\n",
    "                    ]\n",
    "config_triplets_pclam = [\n",
    "    # `                   ['back_forth', 'n_back_forth', 100],\n",
    "    #                      ['back_forth','scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 10],\n",
    "                    # ['clamiter_init', 'dim_attr',70]] # best so far\n",
    "                    ]\n",
    "# best score here 0.69, sometimes 0.66 with prior. fat\n",
    "\n",
    "ds_name = 'ellipticGGAD'\n",
    "#! why does prior go up but prior star doesn't?\n",
    "\n",
    "#todo: try use_fat, best or the one that is best, with optimize vanilla and without with first func feats/prior\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        attr_opt=True, #! clamiter\n",
    "                                        optimize_vanilla=False, #! anomaly config\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=False,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "#todo: if there is an error print it and return the best value. make sure it's not inf or any of that nonsense...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "\n",
    "config_triplets_piegam = [\n",
    "                            # ['back_forth', 'n_back_forth', 40],\n",
    "                            # ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 40], \n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                    ] # best so far\n",
    "\n",
    "\n",
    "#got 58 with bigclam\n",
    "config_triplets_pclam = [\n",
    "                        # ['back_forth', 'n_back_forth', 40],\n",
    "                        #  ['back_forth', 'scheduler_step_size', 1],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 1000],\n",
    "                    # ['prior_opt', 'lr', 0.00001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.05],\n",
    "                    # ['feat_opt' , 'n_iter', 100],\n",
    "                    # ['feat_opt', 'lr', 0.00001],\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'dim_feat', 20], # try even less, this is the lowest value i've tried\n",
    "                    # ['clamiter_init', 'dim_attr', 100]\n",
    "                    ] # best so far\n",
    "# PHOTO\n",
    "ds_name = 'photoGGAD'\n",
    "# raw attr for photoGGAD is 745 dimension\n",
    "# 0.60 is good here\n",
    "#todo: try use_fat, best or the one that is best, with optimize vanilla and without with first func feats/prior\n",
    "#PCALM does very good!! for some reason, on this dataset, prior improves but prior star doesn't change... vanilla doesn't change also...\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None, \n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='best',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: test the greedy anomaly detection algorithm!\n",
    "#************* SCHEDULER BABY YAHHHH *************** and remove the limitations from\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 20000]]\n",
    "config_triplets_piegam = [\n",
    "                    # ['back_forth', 'n_back_forth', 40],\n",
    "                    # ['back_forth', 'scheduler_step_size', 3],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "\n",
    "config_triplets_pclam = [\n",
    "                    # ['back_forth', 'n_back_forth', 40],\n",
    "                    # ['back_forth', 'scheduler_step_size', 3],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    # ['prior_opt', 'n_iter', 10000],\n",
    "                    # ['prior_opt', 'lr', 0.0001],\n",
    "                    # ['prior_opt', 'noise_amp', 0.005], #! if last thing better than 0.649 and 0.53 this was 0.01\n",
    "                    # ['feat_opt' , 'n_iter', 1000],\n",
    "                    # ['feat_opt', 'lr', 0.000001], # good for when starting with trained piegam\n",
    "                    # ['clamiter_init', 'l1_reg', 1],\n",
    "                    # ['clamiter_init', 'hidden_dim', 32], #! this i a;so changed for last run\n",
    "                    # ['clamiter_init', 'num_coupling_blocks', 32],\n",
    "                    # ['clamiter_init', 'num_layers_mlp', 4],\n",
    "                    # ['clamiter_init', 'dim_attr', 40]\n",
    "                    ] # best so far\n",
    "# extra config for successful piegam, also there are no bounds for best auc.\n",
    "\n",
    "\n",
    "ds_name = 'redditGGAD'\n",
    "#todo: i want to have the reddit dataset without optimi\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_piegam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        performance_metric='best', \n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=False)\n",
    "                                        #! problem in assert graph is undirected.\n",
    "\n",
    "\n",
    "losses = ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam',              \n",
    "                                        attr_opt=True,\n",
    "                                        optimize_vanilla=True,\n",
    "                                        optimize_prior=True,\n",
    "                                        config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_pclam,\n",
    "                                        # external_ds = trainer_clam.data,\n",
    "                                        external_ds = None,\n",
    "                                        performance_metric='prior_star',\n",
    "                                        verbose=True,\n",
    "                                        verbose_in_funcs=True,\n",
    "                                        use_fat=True)\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first optimize vanilla so that no need to optimize it over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA -- (before the newest optimization)\n",
    "\n",
    "reddit_ds_with_anomalies = import_dataset('redditGGAD')\n",
    "\n",
    "fat_reddit_ds_with_anomalies = TwoHop()(reddit_ds_with_anomalies)\n",
    "fat_reddit_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_reddit_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    early_stop_fit=50,\n",
    "    verbose=True)\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfFINANCE \n",
    "i think i can do without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNSUPERVISED VANILLA --\n",
    "\n",
    "tfFinance_ds_with_anomalies = import_dataset('tfFinanceGGAD')\n",
    "\n",
    "fat_tfFinance_ds_with_anomalies = TwoHop()(tfFinance_ds_with_anomalies)\n",
    "fat_tfFinance_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_tfFinance_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 20000]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=tfFinance_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam, accuracies, best_accuracy = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True,\n",
    "    verbose=True)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['prior_opt', 'noise_amp', 0.1],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'l1_reg', 1],\n",
    "                    ['clamiter_init', 'dim_attr', 10]] # best so far\n",
    "ds_name = 'tfFinanceGGAD'\n",
    "ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'pclam', \n",
    "                                        external_ds=None,\n",
    "                                        attr_opt=True,config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_p,\n",
    "                                        use_fat=False, \n",
    "                                        optimize_vanilla=False)\n",
    "\n",
    "ad.anomaly_detection_unsupervised_priored(ds_name, \n",
    "                                        'piegam', \n",
    "                                        external_ds=None,\n",
    "                                        attr_opt=True,config_triplets_gam=config_triplets_gam, \n",
    "                                        config_triplets_prior=config_triplets_p, \n",
    "                                        use_fat=False,\n",
    "                                        optimize_vanilla=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUP PIEGAM BLOGCATALOG\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'dim_attr', 100]] # best so far\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='pclam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    inflation_flow_name=None,\n",
    "    attr_opt=True,\n",
    "    \n",
    "    device=device)\n",
    "#! problem with prior need to make the weights bigger.need to make this into a feature because it depends on the prior weights\n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_feats',\n",
    "            auc_every=1,\n",
    "            early_stop=3,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#? maybe stop the feat opt after a crtain point?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=blogcatalog_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[7, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=blogcatalog_ds, model_name='iegam', \n",
    "    ks=[6, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFICATION\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_blogcatalog_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[7, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=True)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_blogcatalog_ds, model_name='iegam', \n",
    "    ks=[6, 10, 14, 16, 18, 20, 22, 24,26, 28, 30, 32, 34, 36, 38, 40], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=True)\n",
    "\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n",
    "#todo: 2 communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "acm_ds = import_dataset('ACM', verbose=True)\n",
    "\n",
    "fat_acm_ds = TwoHop()(acm_ds)\n",
    "fat_acm_ds.edge_attr = torch.ones(\n",
    "                fat_acm_ds.edge_attr.shape[0])\n",
    "acm_ds.attr\n",
    "#? doing mask to index and index to mask to remove isolated nodes in dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised acm\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam_acm = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=acm_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam_acm.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 200],\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],]\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam_acm, trainer_gam_acm.data, verbose=True)\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam_acm.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "losses_piegam = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "\n",
    "plot_losses(losses_piegam[0], losses_piegam[1])\n",
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "# could it be that i removed nodes and then the gt nomalous changed?\n",
    "\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=acm_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=acm_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "# can we know which anomalies are feature based and which topological?\n",
    "# todo: check if the labeling changes when you remove nodes.\n",
    "blogcatalog_ds = import_dataset('BlogCatalog', attr_transform='truncated_svd', n_components=100)\n",
    "\n",
    "fat_blogcatalog_ds = TwoHop()(blogcatalog_ds)\n",
    "fat_blogcatalog_ds.edge_attr = torch.ones(\n",
    "                fat_blogcatalog_ds.edge_attr.shape[0])\n",
    "\n",
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogcatalog_ds.attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised blogcatalog\n",
    "\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 1300]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=blogcatalog_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "\n",
    "auc_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "\n",
    "#todo: can also try to first fit iegam no prior and then do the 100 back forth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUP PIEGAM BLOGCATALOG\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 40],\n",
    "                     # ['prior_opt', 'n_iter', 1000], good results for prior star\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],\n",
    "                    ['feat_opt', 'lr', 0.00001],\n",
    "                    ['clamiter_init', 'dim_attr', 100]] # best so far\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    inflation_flow_name=None,\n",
    "    attr_opt=True,\n",
    "    \n",
    "    device=device)\n",
    "#! problem with prior need to make the weights bigger.need to make this into a feature because it depends on the prior weights\n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_feats',\n",
    "            auc_every=1,\n",
    "            early_stop=3,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#? maybe stop the feat opt after a crtain point?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFICATION\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_acm_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_acm_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=2000,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the difference between the topological structured data can also help to see if the feature detection is good since maybe sometimes you need a language model when you just use words and stuff\n",
    "\n",
    "# todo things to try: partial densification\n",
    "flickr_ds = import_dataset('Flickr')\n",
    "\n",
    "fat_flickr_ds = TwoHop()(flickr_ds)\n",
    "fat_flickr_ds.edge_attr = torch.ones(\n",
    "                fat_flickr_ds.edge_attr.shape[0])\n",
    "\n",
    "flickr_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieclam unsupervised acm\n",
    "#todo: make this into a function! \n",
    "#todo: make function\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=flickr_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "#todo: check while training sometimes classify unsupervised...\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 200],\n",
    "                    ['prior_opt', 'n_iter', 1000],\n",
    "                    ['feat_opt' , 'n_iter', 10],]\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "losses_piegam = trainer_piegam.train_model_on_params(\n",
    "            init_feats=True, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "\n",
    "plot_losses(losses_piegam[0], losses_piegam[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DENSIFICATION\n",
    "# make a test for the server to use prior or stuff... don't know shit. prior unsupervised\n",
    "ks_aucs_bigclam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=flickr_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_sparse = ad.cross_val_communities_unsupervised(\n",
    "    ds=flickr_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "#todo: try less iterations, semi supervised method...\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_sparse[0], ks_aucs_bigclam_sparse[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_sparse[0], ks_aucs_iegam_sparse[1], label='iegam')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIFIED\n",
    "ks_aucs_bigclam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_flickr_ds, \n",
    "    model_name='bigclam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "ks_aucs_iegam_dense = ad.cross_val_communities_unsupervised(\n",
    "    ds=fat_flickr_ds, model_name='iegam', \n",
    "    ks=[10, 14, 16, 18, 20, 22, 24,26, 28, 30], \n",
    "    n_iter=1700,\n",
    "    device=device,\n",
    "    verbose=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_bigclam_dense[0], ks_aucs_bigclam_dense[1], label='bigclam')\n",
    "plt.scatter(ks_aucs_iegam_dense[0], ks_aucs_iegam_dense[1], label='iegam')\n",
    "plt.legend()\n",
    "#! maybe edge attr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGAD datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigclam: 18 communities, 1700 iters\n",
    "iegam: 24 communities, 2000 iters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reddit_ds_with_anomalies = import_dataset('redditGGAD')\n",
    "\n",
    "fat_reddit_ds_with_anomalies = TwoHop()(reddit_ds_with_anomalies)\n",
    "fat_reddit_ds_with_anomalies.edge_attr = torch.ones(\n",
    "                fat_reddit_ds_with_anomalies.edge_index.shape[1])\n",
    "\n",
    "# reddit_ds_with_anomalies.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED bigclam\n",
    "# ============\n",
    "\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 1700]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    attr_opt=True,\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, ret_ap=True, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_triplets_p = [\n",
    "                     ['feat_opt', 'n_iter', 10],\n",
    "                     ['prior_opt', 'n_iter', 1000],\n",
    "                     ['back_forth', 'n_back_forth', 100],\n",
    "                     ]\n",
    "trainer_piegam = Trainer( \n",
    "            model_name='pclam', \n",
    "            dataset=trainer_clam.data.clone(),\n",
    "            config_triplets_to_change=config_triplets_p,\n",
    "            inflation_flow_name=None,\n",
    "            attr_opt=True,\n",
    "            device=device)\n",
    "        \n",
    "losses = trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            auc_every=1,\n",
    "            early_stop=2,\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "        \n",
    "\n",
    "plot_losses(losses[0], losses[1])\n",
    "plot_auc_dict(losses[2])\n",
    "#TODO: for this situation, need to make the early stop checks at the beginning so it doesn't get ruined by the back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_vanilla = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='vanilla_star')\n",
    "\n",
    "auc_prior = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='prior')\n",
    "\n",
    "auc_prior_star = ad.classify_unsupervised(trainer_piegam, trainer_piegam.data, ll_type='prior_star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "# gam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIGCLAM UNSUPERVISED\n",
    "config_triplets_clam = [['feat_opt', 'n_iter', 1700]]\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_clam = trainer_clam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "auc_bigclam, ap_bigclam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, verbose=True)\n",
    "\n",
    "# UNSUPERVISED clam\n",
    "# ============\n",
    "#todo: congrads! prior improves on unsupervised! there is some problem with star probs being infinite... take care of it.\n",
    "#* bigclam auc was 0.57 and improved with prior to 0.62\n",
    "config_triplets_clam = []\n",
    "# -- Train Base ---\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_clam,\n",
    "    device=device)\n",
    "\n",
    "losses_bigclam = trainer_clam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='small_gaus')[0]\n",
    "plot_losses(losses_bigclam) # GPU one round added 10000 mib...\n",
    "auc_bigclam, ap_bigclam = ad.classify_unsupervised(trainer_clam, trainer_clam.data, verbose=True)\n",
    "\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "\n",
    "# -- Train With Prior --\n",
    "trainer_pclam = Trainer( \n",
    "    model_name='pclam', \n",
    "    dataset=trainer_clam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_pclam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_pclam, \n",
    "    trainer_pclam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n",
    "\n",
    "#! make unsupervised and semi supervised option for the trainer?\n",
    "#! try special densification?\n",
    "# options on how to make semi supervised:\n",
    "# 1. train all nodes then teach prior on the train labeled set\n",
    "# 2. densify in the special way the train labeled set \n",
    "# do just unsupervised piegam: train the prior on everything\n",
    "# then do semi supervised in two versions: densify everything train prior then train bigclam\n",
    "#todo: need to put a mask. maybe divide in the anomaly detector into supervised and semi supervised...\n",
    "#todo: question: if i initialize a trainer with a dataset that has features what happens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elliptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_ds = import_dataset('ellipticGGAD')\n",
    "\n",
    "fat_elliptic_ds = TwoHop()(elliptic_ds)\n",
    "fat_elliptic_ds.edge_attr = torch.ones(\n",
    "                fat_elliptic_ds.edge_attr.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_elliptic_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True, ret_ap=True)\n",
    "# gam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_ds = import_dataset('photoGGAD')\n",
    "\n",
    "fat_photo_ds = TwoHop()(photo_ds)\n",
    "fat_photo_ds.edge_attr = torch.ones(\n",
    "                fat_photo_ds.edge_attr.shape[0])\n",
    "\n",
    "photo_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_photo_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfFinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfFinance_ds = import_dataset('photoGGAD')\n",
    "\n",
    "fat_tfFinance_ds = TwoHop()(tfFinance_ds)\n",
    "fat_tfFinance_ds.edge_attr = torch.ones(\n",
    "                fat_tfFinance_ds.edge_attr.shape[0])\n",
    "\n",
    "tfFinance_ds.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=tfFinance_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_ds = import_dataset('amazonGGAD')\n",
    "\n",
    "fat_amazon_ds = TwoHop()(amazon_ds)\n",
    "fat_amazon_ds.edge_attr = torch.ones(\n",
    "                fat_amazon_ds.edge_attr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED\n",
    "# ============\n",
    "#! no memory! check why\n",
    "# unsupervised iegam\n",
    "config_triplets_gam = [['feat_opt', 'n_iter', 2000]]\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_amazon_ds.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "\n",
    "losses_gam = trainer_gam.train_model_on_params(\n",
    "    init_type='small_gaus', \n",
    "    init_feats=True)\n",
    "\n",
    "\n",
    "auc_gam, ap_gam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED GAM\n",
    "# ============\n",
    "'''train prior on all nodes without second clam training (didn't work when i tried)'''\n",
    "config_triplets_gam = []\n",
    "# -- Train Base ---\n",
    "trainer_gam = Trainer(\n",
    "    model_name='iegam', \n",
    "    dataset=fat_reddit_ds_with_anomalies.clone(),\n",
    "    config_triplets_to_change=config_triplets_gam,\n",
    "    device=device)\n",
    "\n",
    "losses_iegam = trainer_gam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='small_gaus')[0]\n",
    "plot_losses(losses_iegam) # GPU one round added 10000 mib...\n",
    "auc_iegam, ap_iegam = ad.classify_unsupervised(trainer_gam, trainer_gam.data, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Train With Prior --\n",
    "config_triplets_p = [['back_forth', 'n_back_forth', 0]]\n",
    "trainer_piegam = Trainer( \n",
    "    model_name='piegam', \n",
    "    dataset=trainer_gam.data.clone(),\n",
    "    config_triplets_to_change=config_triplets_p,\n",
    "    device=device)\n",
    "\n",
    "trainer_piegam.train_model_on_params(\n",
    "            init_feats=False, \n",
    "            first_func_in_fit='fit_prior',\n",
    "            verbose=False, \n",
    "            plot_every=10000)\n",
    "\n",
    "# --- Classify Nodes ---\n",
    "auc_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='vanilla_star', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior', \n",
    "    verbose=True)\n",
    "\n",
    "auc_prior_star, _ = ad.classify_unsupervised(\n",
    "    trainer_piegam, \n",
    "    trainer_piegam.data, \n",
    "    ll_type='prior_star', \n",
    "    verbose=True)\n",
    "# ======================================\n",
    "\n",
    "#! make unsupervised and semi supervised option for the trainer?\n",
    "#! try special densification?\n",
    "# options on how to make semi supervised:\n",
    "# 1. train all nodes then teach prior on the train labeled set\n",
    "# 2. densify in the special way the train labeled set \n",
    "# do just unsupervised piegam: train the prior on everything\n",
    "# then do semi supervised in two versions: densify everything train prior then train bigclam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMAL NEIGH EXPERIMENT #! not very good....\n",
    "# ======================== #! takes a super long time and doesn't improve anything\n",
    "\n",
    "\n",
    "config_triplet = []\n",
    "\n",
    "trainer_clam = Trainer(\n",
    "    model_name='bigclam', \n",
    "    dataset=fat_reddit_ds_with_anomalies,\n",
    "    config_triplets_to_change=config_triplet,\n",
    "    device=device)\n",
    "\n",
    "losses_bigclam = trainer_clam.train_model_on_params(\n",
    "    init_feats=True, \n",
    "    init_type='minimal_neigh')[0]\n",
    "plot_losses(losses_bigclam) # GPU one round added 10000 mib...\n",
    "auc, ap = ad.classify_unsupervised(trainer_clam, fat_reddit_ds_with_anomalies, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to plot scatter 2000 vs 3000 to see if there is a diffrernce\n",
    "# seems like 2000 is better.... but can't relly know\n",
    "ks_aucs_ie_comb = np.concatenate([ks_aucs_aps_iegam_2000, ks_aucs_aps_iegam_3000], axis=1)\n",
    "ks_aucs_bc_comb = np.concatenate([ks_aucs_aps_bigclam_2000, ks_aucs_aps_bigclam_3000], axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_iegam_2000[0], ks_aucs_aps_iegam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_bigclam_2000[0], ks_aucs_aps_bigclam_2000[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_ie_comb[0], ks_aucs_ie_comb[1])\n",
    "plt.scatter(ks_aucs_bc_comb[0], ks_aucs_bc_comb[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_bigclam_2000[0], ks_aucs_aps_bigclam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_bigclam_3000[0], ks_aucs_aps_bigclam_3000[1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ks_aucs_aps_iegam_2000[0], ks_aucs_aps_iegam_2000[1])\n",
    "plt.scatter(ks_aucs_aps_iegam_3000[0], ks_aucs_aps_iegam_3000[1])\n",
    "\n",
    "\n",
    "\n",
    "#todo: save the 2000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_aucs_aps_iegam_2000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'iegam', [16, 16, 18,18, 20, 20, 22, 22, 24,24], 2000, device=device, return_ap=True)\n",
    "\n",
    "ks_aucs_aps_iegam_3000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'iegam', [16, 16, 18,18, 20, 20, 22, 22, 24,24], 3000, device=device, return_ap=True)\n",
    "\n",
    "\n",
    "\n",
    "ks_aucs_aps_bigclam_2000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'bigclam', [16, 16,17, 17, 18,18, 19,19,20,20,21,21,22,22] , 2000, device=device, return_ap=True)\n",
    "\n",
    "ks_aucs_aps_bigclam_3000 = ad.cross_val_communities(fat_reddit_ds_with_anomalies, 'bigclam', [16, 16,17, 17, 18,18, 19,19,20,20,21,21,22,22], 3000, device=device, return_ap=True)\n",
    "\n",
    "#todo: compare number of iterations and see if there is a significant difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_prior = ad.classify_test_set_no_trainer(fat_reddit_ds_with_anomalies, prior, ll_type='prior')[0]\n",
    "auc_prior_star = ad.classify_test_set_no_trainer(fat_reddit_ds_with_anomalies, prior, ll_type='prior_star')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_prediction_adj(trainer_bigclam.data, lorenz=False)\n",
    "plot_adj(fb_data_with_anomalies.edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and train unsupervised:\n",
    "# train bigclam and classify vanilla star\n",
    "# make the fat ds with anomalies with all of the train test labels and stuff just init all of the node features. maybe even just gaussian for everything. then initialize trainer with the dataset\n",
    "fb_data = import_dataset('facebook348')\n",
    "fb_data_with_anomalies = ad.add_anomalies(fb_data, add_method='uniform', avg_deg_factor=1)\n",
    "#densify the fb_data_with_anomalies\n",
    "fat_fb_data_with_anomalies = TwoHop()(fb_data_with_anomalies)\n",
    "\n",
    "trainer_bigclam = Trainer(\n",
    "    dataset_name='facebook348', \n",
    "    model_name='bigclam', \n",
    "    dataset=fat_fb_data_with_anomalies,\n",
    "    device=device)\n",
    "\n",
    "# now train the trainer and run vanilla classification to see if we get weird places for anomalies\n",
    "\n",
    "losses_bigclam = trainer_bigclam.train_model_on_params()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc, ll_normal,ll_anomalies = ad.classify_unsupervised(\n",
    "    trainer_bigclam, \n",
    "    trainer_bigclam.data,\n",
    "    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piegam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
