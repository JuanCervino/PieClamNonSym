

# 88""Yb 88  88  dP"Yb  888888  dP"Yb  
# 88__dP 88  88 dP   Yb   88   dP   Yb 
# 88"""  888888 Yb   dP   88   Yb   dP 
# 88     88  88  YbodP    88    YbodP  

photo_bigclam:
    clamiter_init:
        dim_feat: 46
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000001
        n_iter: 7000
        

photo_iegam:
    clamiter_init:
        dim_feat: 56
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000001
        n_iter: 10000
        

photo_pclam:
    clamiter_init:
        dim_feat: 46
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.00001
        n_iter: 100
        
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_feats'
        

photo_piegam:
    clamiter_init:
       dim_feat : 56
       s_reg : 0.0
    feat_opt:
        lr: 0.00001
        n_iter: 500
    prior_opt:
        lr: 0.00001
        n_iter: 1000
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        first_func_in_fit: 'fit_feats'



#  dP""b8  dP"Yb  88""Yb    db    
# dP   `" dP   Yb 88__dP   dPYb   
# Yb      Yb   dP 88"Yb   dP__Yb  
#  YboodP  YbodP  88  Yb dP""""Yb 

cora_bigclam:
    clamiter_init:
        dim_feat: 46
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
        T: 1
    feat_opt:
        lr: 0.000001
        n_iter: 10000
        

cora_iegam:
    clamiter_init:
        dim_feat: 56
        dim_attr: 500
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000001
        n_iter: 10000
        

cora_pclam:
    clamiter_init:
        dim_feat: 46
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        hidden_dim: 64
        num_coupling_blocks: 32 
        num_layers_mlp: 2
    feat_opt:
        lr: 0.00001
        n_iter: 1500
        
    prior_opt:
        lr: 0.00001
        n_iter: 15000
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        scheduler_step_size: 1
        scheduler_gamma: 0.5
        first_func_in_fit: 'fit_feats'
        

cora_piegam:
    clamiter_init:
       dim_feat : 56
       s_reg : 0.0
    feat_opt:
        lr: 0.00001
        n_iter: 1500
    prior_opt:
        lr: 0.00001
        n_iter: 1500
        noise_amp: 0.05
        weight_decay: 0.1
        
    back_forth:
        n_back_forth: 100
        first_func_in_fit: 'fit_feats'

# 888888 888888 Yb  dP    db    .dP"Y8 
#   88   88__    YbdP    dPYb   `Ybo." 
#   88   88""    dPYb   dP__Yb  o.`Y8b 
#   88   888888 dP  Yb dP""""Yb 8bodP' 
                                     
                                     
     
texas_iegam:
    clamiter_init:
        dim_feat: 24
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000003
        n_iter: 13000
                                        

texas_bigclam:
    clamiter_init:
        dim_feat: 24
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000003
        n_iter: 13000
                         
texas_piegam:
    clamiter_init:
        dim_feat: 22
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000004
        n_iter: 2000
    back_forth:
        first_func_in_fit: 'fit_prior'
                         

texas_pclam:
    clamiter_init:
        dim_feat: 24
        s_reg: 0.0
        l1_reg: 1
    feat_opt:
        lr: 0.000003
        n_iter: 1000
    back_forth:
        first_func_in_fit: 'fit_prior'



# interesting values:
# 0.01, 32, 32, 2: give more iterations in the second classify anomaly. seems to not saturate. seems that way also after many tries! not so sure that that is true. for 0.5 i think that 3 layer mlp might have been better
# what happens when the loss climbs really high? is it good for anomaly detection? 
# these numbers are the first time i went over the base in 0.2! and maybe can still be made better!


#  dP""b8 88      dP"Yb  88""Yb    db    88     
# dP   `" 88     dP   Yb 88__dP   dPYb   88     
# Yb  "88 88  .o Yb   dP 88""Yb  dP__Yb  88  .o 
#  YboodP 88ood8  YbodP  88oodP dP""""Yb 88ood8 


MightyConfigs_iegam:
    clamiter_init: 
        dim_feat: 56
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000001
        n_iter: 7000
        early_stop: 0
    

MightyConfigs_piegam:
    clamiter_init: 
        dim_feat: 56
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000003
        n_iter: 500
        early_stop: 0
    prior_opt:
        n_iter: 1300
        lr: 0.000002
        noise_amp: 0.05
        weight_decay: 0.1
        early_stop: 0
    back_forth:
        n_back_forth: 6
        scheduler_step_size: 3
        scheduler_gamma: 0.5
        early_stop_fit: 0
        first_func_in_fit: "fit_feats"


MightyConfigs_bigclam:
    clamiter_init: 
        dim_feat: 40
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000001
        n_iter: 7000
        early_stop: 0


MightyConfigs_pclam:
    clamiter_init: 
        dim_feat: 40
        dim_attr: 100
        s_reg: 0.0
        l1_reg: 1
        T: 1
        hidden_dim: 64
        num_coupling_blocks: 32
        num_layers_mlp: 2
    feat_opt: 
        lr: 0.000003
        n_iter: 500
        early_stop: 0
    prior_opt:
        n_iter: 1300
        lr: 0.0000002
        noise_amp: 0.1
        weight_decay: 0.1
        early_stop: 0
    back_forth:
        n_back_forth: 6
        scheduler_step_size: 3
        scheduler_gamma: 0.5
        early_stop_fit: 0
        first_func_in_fit: "fit_feats"
