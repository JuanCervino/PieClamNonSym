{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load the model from the file \"sbm3x3_pclam_roc_0.210_auc_0.860\"\n",
    "import torch\n",
    "from torch_geometric.transforms import TwoHop\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "from datasets.import_dataset import import_dataset\n",
    "from utils.plotting import *\n",
    "from trainer import Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../datasets/import_dataset.py:410: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  edge_index = torch.tensor(adj.nonzero(), dtype=torch.long)\n",
      "/home/user/anaconda3/envs/piegam/lib/python3.11/site-packages/torch_geometric/edge_index.py:784: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../trainer.py:311:::  \n",
      " starting optimization of piegam on ellipticGGAD on device cuda\n",
      "\n",
      " configs_dict: \n",
      "{\n",
      "    \"clamiter_init\": {\n",
      "        \"dim_feat\": 30,\n",
      "        \"dim_attr\": 93,\n",
      "        \"s_reg\": 0.0,\n",
      "        \"l1_reg\": 1,\n",
      "        \"T\": 1,\n",
      "        \"hidden_dim\": 64,\n",
      "        \"num_coupling_blocks\": 32,\n",
      "        \"num_layers_mlp\": 2\n",
      "    },\n",
      "    \"feat_opt\": {\n",
      "        \"lr\": 3e-06,\n",
      "        \"n_iter\": 500,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"prior_opt\": {\n",
      "        \"n_iter\": 1300,\n",
      "        \"lr\": 2e-06,\n",
      "        \"noise_amp\": 0.05,\n",
      "        \"weight_decay\": 0.1,\n",
      "        \"early_stop\": 0\n",
      "    },\n",
      "    \"back_forth\": {\n",
      "        \"n_back_forth\": 3,\n",
      "        \"scheduler_step_size\": 1,\n",
      "        \"scheduler_gamma\": 0.5,\n",
      "        \"early_stop_fit\": 0,\n",
      "        \"first_func_in_fit\": \"fit_feats\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../trainer.py:316:::  \n",
      " train_model_on_params, initializing feats with small_gaus\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../trainer.py:324:::  \n",
      " init_node_feats took 0.003964424133300781 seconds\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:501:::  \n",
      "fit, task='anomaly'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:546:::  \n",
      "in fit,\n",
      "first_func_in_fit='fit_feats'\n",
      "second_function_name='fit_prior'\n",
      "\n",
      "\n",
      "/home/user/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:604:::  \n",
      "back and forth 1/3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 43.44 MiB is free. Process 3784506 has 13.58 GiB memory in use. Process 3835235 has 558.00 MiB memory in use. Process 3836266 has 540.00 MiB memory in use. Process 907369 has 478.00 MiB memory in use. Including non-PyTorch memory, this process has 8.00 GiB memory in use. Of the allocated memory 3.71 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     23\u001b[0m config_triplets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m trainer_anomaly \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     26\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     27\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     config_triplets_to_change\u001b[38;5;241m=\u001b[39mconfig_triplets\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m losses, acc_test, acc_val \u001b[38;5;241m=\u001b[39m trainer_anomaly\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     36\u001b[0m     init_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmall_gaus\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m     init_feats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m     acc_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     39\u001b[0m     plot_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     40\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m     verbose_in_funcs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m losseses\u001b[38;5;241m.\u001b[39mappend(losses)\n\u001b[1;32m     44\u001b[0m acc_testses\u001b[38;5;241m.\u001b[39mappend(acc_test)\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../trainer.py:380\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, init_type, dyads_to_omit, task_params, init_feats, acc_every, performance_metric, prior_fit_mask, plot_every, verbose, verbose_in_funcs, node_feats_for_init, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \n\u001b[1;32m    368\u001b[0m             step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mback_forth\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler_step_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    369\u001b[0m             gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mback_forth\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler_gamma\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    371\u001b[0m fit_opt_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_params\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_opt\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprior_params\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprior_opt\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stop_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mback_forth\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stop_fit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    378\u001b[0m         }\n\u001b[0;32m--> 380\u001b[0m losses, accuracies_test, accuracies_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamiter\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    381\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \n\u001b[1;32m    382\u001b[0m                         optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler,\n\u001b[1;32m    383\u001b[0m                         dyads_to_omit\u001b[38;5;241m=\u001b[39mdyads_to_omit, \n\u001b[1;32m    384\u001b[0m                         prior_fit_mask\u001b[38;5;241m=\u001b[39mprior_fit_mask,\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m                         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, \n\u001b[1;32m    387\u001b[0m                         acc_every\u001b[38;5;241m=\u001b[39macc_every, \n\u001b[1;32m    388\u001b[0m                         performance_metric\u001b[38;5;241m=\u001b[39mperformance_metric,\n\u001b[1;32m    389\u001b[0m                         configs_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs_dict,\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m                         plot_final_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    392\u001b[0m                         plot_every\u001b[38;5;241m=\u001b[39mplot_every,\n\u001b[1;32m    393\u001b[0m                         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    394\u001b[0m                         verbose_in_funcs\u001b[38;5;241m=\u001b[39mverbose_in_funcs,\n\u001b[1;32m    395\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_opt_params,\n\u001b[1;32m    396\u001b[0m                         \n\u001b[1;32m    397\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    401\u001b[0m     printd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrain_model_on_params on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt_train_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:610\u001b[0m, in \u001b[0;36mPCLAMIter.fit\u001b[0;34m(self, graph, first_func_in_fit, optimizer, scheduler, n_back_forth, dyads_to_omit, prior_fit_mask, plot_every, acc_every, early_stop_fit, early_stops, performance_metric, configs_dict, task, verbose_in_funcs, verbose, **params)\u001b[0m\n\u001b[1;32m    608\u001b[0m t_first \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     losses_epoch_1st, accuracies_test_epoch_1st, accuracies_val_epoch_1st \u001b[38;5;241m=\u001b[39m first_func()\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    612\u001b[0m     printd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfit func. nerror in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_func_in_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:522\u001b[0m, in \u001b[0;36mPCLAMIter.fit.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    518\u001b[0m     prior_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stop\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m early_stops[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    519\u001b[0m     feat_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stop\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m early_stops[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 522\u001b[0m fit_feats_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_feats(\n\u001b[1;32m    523\u001b[0m                     graph, \n\u001b[1;32m    524\u001b[0m                     dyads_to_omit\u001b[38;5;241m=\u001b[39mdyads_to_omit, \n\u001b[1;32m    525\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39mverbose_in_funcs,\n\u001b[1;32m    526\u001b[0m                     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    527\u001b[0m                     acc_every\u001b[38;5;241m=\u001b[39macc_every,\n\u001b[1;32m    528\u001b[0m                     performance_metric\u001b[38;5;241m=\u001b[39mperformance_metric,\n\u001b[1;32m    529\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfeat_params,\n\u001b[1;32m    530\u001b[0m                     kwargs\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    532\u001b[0m fit_prior_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_prior(\n\u001b[1;32m    533\u001b[0m                     graph\u001b[38;5;241m=\u001b[39mgraph, \n\u001b[1;32m    534\u001b[0m                     node_mask\u001b[38;5;241m=\u001b[39mprior_fit_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprior_params,\n\u001b[1;32m    541\u001b[0m                     kwargs\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# FIRST AND SECOND PARAMS\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:414\u001b[0m, in \u001b[0;36mPCLAMIter.fit_feats\u001b[0;34m(self, graph, n_iter, lr, task, node_mask, performance_metric, dyads_to_omit, early_stop, cutoff, verbose, acc_every, plot_every, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_feats\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    397\u001b[0m               graph, \n\u001b[1;32m    398\u001b[0m               n_iter, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m               plot_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m,\n\u001b[1;32m    409\u001b[0m               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''optimize the features using iterations of clamiter.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    param: node_mask: a mask for the nodes to optimize, the rest should stay unchanged'''\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_wrapper(graph\u001b[38;5;241m=\u001b[39mgraph,\n\u001b[1;32m    415\u001b[0m                             n_iter\u001b[38;5;241m=\u001b[39mn_iter, \n\u001b[1;32m    416\u001b[0m                             lr\u001b[38;5;241m=\u001b[39mlr, \n\u001b[1;32m    417\u001b[0m                             task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    418\u001b[0m                             which_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_feats\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    419\u001b[0m                             node_mask\u001b[38;5;241m=\u001b[39mnode_mask,\n\u001b[1;32m    420\u001b[0m                             performance_metric\u001b[38;5;241m=\u001b[39mperformance_metric,\n\u001b[1;32m    421\u001b[0m                             dyads_to_omit\u001b[38;5;241m=\u001b[39mdyads_to_omit, \n\u001b[1;32m    422\u001b[0m                             early_stop\u001b[38;5;241m=\u001b[39mearly_stop,\n\u001b[1;32m    423\u001b[0m                             cutoff\u001b[38;5;241m=\u001b[39mcutoff, \n\u001b[1;32m    424\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    425\u001b[0m                             acc_every\u001b[38;5;241m=\u001b[39macc_every, \n\u001b[1;32m    426\u001b[0m                             plot_every\u001b[38;5;241m=\u001b[39mplot_every,\n\u001b[1;32m    427\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    428\u001b[0m                             )\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:351\u001b[0m, in \u001b[0;36mPCLAMIter.fit_wrapper\u001b[0;34m(self, graph, n_iter, lr, task, which_fit, early_stop, cutoff, verbose, acc_every, plot_every, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m         loss \u001b[38;5;241m=\u001b[39m iter_step()\n\u001b[1;32m    352\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:273\u001b[0m, in \u001b[0;36mPCLAMIter.fit_wrapper.<locals>.iter_step_feat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_step_feat\u001b[39m():\n\u001b[0;32m--> 273\u001b[0m     clamiter_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(graph, node_mask)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_last_grad \u001b[38;5;241m=\u001b[39m clamiter_grad\n\u001b[1;32m    275\u001b[0m     graph\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_bounding(graph\u001b[38;5;241m.\u001b[39mx, clamiter_grad, lr, node_mask, cutoff), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5000\u001b[39m,\u001b[38;5;241m5000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/piegam/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/piegam/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:159\u001b[0m, in \u001b[0;36mPCLAMIter.forward\u001b[0;34m(self, graph, node_mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# attributes must not change\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 159\u001b[0m     tbr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index\u001b[38;5;241m=\u001b[39mgraph\u001b[38;5;241m.\u001b[39medge_index, x\u001b[38;5;241m=\u001b[39mgraph\u001b[38;5;241m.\u001b[39mx, global_features\u001b[38;5;241m=\u001b[39m(prior_grad), edge_attr\u001b[38;5;241m=\u001b[39mgraph\u001b[38;5;241m.\u001b[39medge_attr)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# tbr2 = self.propagate(edge_index=graph.edge_index, x=graph.x, global_features=(torch.tensor(0.0)), edge_attr=graph.edge_attr) + prior_grad\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if not torch.allclose(tbr, tbr2):\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# raise ValueError('tbr and tbr2 not equal')\u001b[39;00m\n\u001b[1;32m    166\u001b[0m tbr \u001b[38;5;241m=\u001b[39m tbr\u001b[38;5;241m*\u001b[39mnode_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/tmp/clamiter_PCLAMIter_propagate_ghr3beld.py:197\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, global_features, edge_attr, size)\u001b[0m\n\u001b[1;32m    185\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    186\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    187\u001b[0m                 x_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 global_features\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mglobal_features,\n\u001b[1;32m    194\u001b[0m             )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage(\n\u001b[1;32m    198\u001b[0m     x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    199\u001b[0m     x_i\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_i,\n\u001b[1;32m    200\u001b[0m     edge_attr\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_attr,\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/Documents/danny/AAAI_pieclam/experiments/../clamiter.py:191\u001b[0m, in \u001b[0;36mPCLAMIter.message\u001b[0;34m(self, x_j, x_i, edge_attr)\u001b[0m\n\u001b[1;32m    188\u001b[0m msg_0 \u001b[38;5;241m=\u001b[39m x_j\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# edge attr is 0 for omitted dyads\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m msg \u001b[38;5;241m=\u001b[39m edge_attr\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mmsg_1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m~\u001b[39medge_attr)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mmsg_0 \n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#? TESTED  torch.where(msg == x_j) == torch.where(edge_attr==0) \u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 43.44 MiB is free. Process 3784506 has 13.58 GiB memory in use. Process 3835235 has 558.00 MiB memory in use. Process 3836266 has 540.00 MiB memory in use. Process 907369 has 478.00 MiB memory in use. Including non-PyTorch memory, this process has 8.00 GiB memory in use. Of the allocated memory 3.71 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "model_names = ['piegam', 'iegam', 'pclam', 'bigclam']\n",
    "ds_names = ['ellipticGGAD', 'redditGGAD', 'photoGGAD']\n",
    "#! problem with something to do with attr with reddit and elliptic\n",
    "for ds_name in ds_names:    \n",
    "    ds = import_dataset(ds_name)\n",
    "    fat_ds = TwoHop()(ds)\n",
    "    fat_ds.edge_attr = torch.ones(fat_ds.edge_index.shape[1]).bool()\n",
    "\n",
    "    if ds_name in ['Flickr', 'ACM', 'BlogCatalog']:\n",
    "        ds_to_use = ds\n",
    "    elif ds_name in ['redditGGAD', 'photoGGAD', 'ellipticGGAD']:\n",
    "        ds_to_use = fat_ds\n",
    "    else:\n",
    "        raise ValueError('ds_name not recognized')\n",
    "\n",
    "    losseses = []\n",
    "    acc_testses = []\n",
    "    acc_valses = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        '''change some of the configs manually e.g. \n",
    "        config_triplets = [['feat_opt', 'n_iter', 1000], ['prior_opt, 'lr', 0.0001], ...]'''\n",
    "        config_triplets = []\n",
    "\n",
    "        trainer_anomaly = Trainer(\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            dataset=ds_to_use.clone(),\n",
    "            attr_opt=True,\n",
    "            task='anomaly',\n",
    "            mighty_configs_dict=True,\n",
    "            config_triplets_to_change=config_triplets\n",
    "        )\n",
    "\n",
    "        losses, acc_test, acc_val = trainer_anomaly.train(\n",
    "            init_type='small_gaus',\n",
    "            init_feats=True,\n",
    "            acc_every=20,\n",
    "            plot_every=-1,\n",
    "            verbose=True,\n",
    "            verbose_in_funcs=False\n",
    "        )\n",
    "        losseses.append(losses)\n",
    "        acc_testses.append(acc_test)\n",
    "        acc_valses.append(acc_val)\n",
    "        \n",
    "        if trainer_anomaly.clamiter.prior is not None:\n",
    "            del trainer_anomaly.clamiter.prior.model\n",
    "        del trainer_anomaly.data\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piegam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
