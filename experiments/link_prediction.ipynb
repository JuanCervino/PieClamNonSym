{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import collections\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# local\n",
    "import optimization_utils as ou\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "from datasets.import_dataset import import_dataset\n",
    "from trainer import Trainer\n",
    "from utils.plotting import *\n",
    "from utils import utils\n",
    "from utils import utils_pyg as up\n",
    "import datasets.simulations as sim\n",
    "import utils.link_prediction as lp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device = {device}')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training goes as follows: \n",
    "1. omit both validation and test sets.\n",
    "2. collect scores from different test sets where the validation set is sampled randomly at every iteration.\n",
    "3. take the maximal validation parameters and train the model on the combined training and validation set.\n",
    "4. report the test score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draft\n",
    "from ogb.linkproppred import Evaluator\n",
    "\n",
    "evaluator = Evaluator(name='ogbl-ddi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDI crossval\n",
    "#DECIDE ON A MODEL AND DATASET\n",
    "model_name = 'ieclam'\n",
    "ds_name = 'ogbl-ddi'\n",
    "\n",
    "# SET RANGE PARAMETERS\n",
    "use_global_config_base = True\n",
    "densify = False\n",
    "attr_opt = False\n",
    "test_p = 0.1\n",
    "val_p = 0.0\n",
    "n_reps = 10\n",
    "\n",
    "'''to do cross validation, give a list of parameters for every one of the parameters as given in the following'''\n",
    "# range_triplets = [\n",
    "#     ['clamiter_init','s_reg', s_regs],\n",
    "#     ['clamiter_init','l1_reg', l1_regs],\n",
    "#     ['clamiter_init', 'dim_feat', dim_feats],\n",
    "#     ['feat_opt','n_iter', n_iters_feats],\n",
    "#     ['feat_opt','lr', lr_feats],\n",
    "# ]\n",
    "\n",
    "# if model_name in ['pclam', 'pieclam']:\n",
    "#     range_triplets += [\n",
    "#         ['back_forth', 'first_func_in_fit', first_funcs_in_fit],\n",
    "#         ['prior_opt','n_iter', n_iters_prior],\n",
    "#         ['prior_opt','lr', lr_prior],\n",
    "#         ['prior_opt','noise_amp', noise_amps],\n",
    "#         ['back_forth','n_back_forth', n_back_forth]\n",
    "#     ]\n",
    "\n",
    "'''another method is to give a list of perturbations to the base config'''\n",
    "#! when i do the deltas the parameters arent saved\n",
    "deltas ={\n",
    "# 'clamiter_init': {'s_reg': 0.001,\n",
    "#                   'l1_reg': 0.001,\n",
    "#                   'dim_feat': 2},\n",
    "# 'feat_opt': {'n_iter': 50,\n",
    "#             'lr': 0.0000001},\n",
    "# 'prior_opt': {'n_iter': 50,\n",
    "#               'lr': 0.0000001,\n",
    "#               'noise_amp': 0.005},\n",
    "'back_forth': {'scheduler_gamma' : 0.1}\n",
    "}\n",
    "\n",
    "range_triplets = ou.perturb_config('anomaly_unsupervised', \n",
    "                                   model_name, \n",
    "                                   deltas, \n",
    "                                   use_global_config=use_global_config_base)\n",
    "\n",
    "\n",
    "\n",
    "#RUN CROSS VALIDATION ON THE DATASET WITH THE MODEL\n",
    "ou.cross_val_link(\n",
    "    ds_name=ds_name,\n",
    "    model_name=model_name,\n",
    "    range_triplets=range_triplets,\n",
    "    use_global_config_base=use_global_config_base,\n",
    "    densify=densify,\n",
    "    attr_opt=attr_opt,\n",
    "    test_p=test_p,\n",
    "    val_p=val_p,\n",
    "    n_reps=n_reps,\n",
    "    device=device\n",
    "    #! add calculation type and finish the hAk function\n",
    "    )\n",
    "#test that texas still works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXAS crossval\n",
    "#DECIDE ON A MODEL AND DATASET\n",
    "\n",
    "\n",
    "# First find parameters on the validation set and use these parameters to train on the training U validation set\n",
    "\n",
    "model_name = 'pieclam'\n",
    "ds_name = 'texas'\n",
    "\n",
    "\n",
    "# SET RANGE PARAMETERS\n",
    "use_global_config_base = True\n",
    "densify = False\n",
    "attr_opt = False\n",
    "test_p = 0.1\n",
    "val_p = 0.05\n",
    "n_reps = 10\n",
    "\n",
    "'''to do cross validation, give a list of parameters for every one of the parameters as given in the following. There is also an option to use the global parameter configuration as shown below. each field is a list of values for that variable'''\n",
    "# range_triplets = [\n",
    "#     ['clamiter_init','s_reg', s_regs],\n",
    "#     ['clamiter_init','l1_reg', l1_regs],\n",
    "#     ['clamiter_init', 'dim_feat', dim_feats],\n",
    "#     ['feat_opt','n_iter', n_iters_feats],\n",
    "#     ['feat_opt','lr', lr_feats],\n",
    "# ]\n",
    "\n",
    "# if model_name in ['pclam', 'pieclam']:\n",
    "#     range_triplets += [\n",
    "#         ['back_forth', 'first_func_in_fit', first_funcs_in_fit],\n",
    "#         ['prior_opt','n_iter', n_iters_prior],\n",
    "#         ['prior_opt','lr', lr_prior],\n",
    "#         ['prior_opt','noise_amp', noise_amps],\n",
    "#         ['back_forth','n_back_forth', n_back_forth]\n",
    "#     ]\n",
    "\n",
    "'''another method is to give a list of perturbations to the base config from which you get the range triplets for the cross val'''\n",
    "\n",
    "deltas ={\n",
    "'clamiter_init': {'s_reg': 0.001,\n",
    "                  'l1_reg': 0.001,\n",
    "                  'dim_feat': 2},\n",
    "'feat_opt': {'n_iter': 50,\n",
    "            'lr': 0.0000001},\n",
    "'prior_opt': {'n_iter': 50,\n",
    "              'lr': 0.0000001,\n",
    "              'noise_amp': 0.005},\n",
    "'back_forth': {'scheduler_gamma' : 0.1}\n",
    "}\n",
    "\n",
    "range_triplets = ou.perturb_config('anomaly_unsupervised', \n",
    "                                   model_name, \n",
    "                                   deltas, \n",
    "                                   use_global_config=use_global_config_base)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RUN CROSS VALIDATION ON THE DATASET WITH THE MODEL\n",
    "#todo: if it's a validation task save to validation folder if it's a test task save to test folder\n",
    "#todo: add the metric folder split. /results/task/data/model/metric/test or val\n",
    "#todo: how does the accuracy saving work? add verbose\n",
    "\n",
    "#todo: make a different accuracy collecting scheme that tells\n",
    "ou.cross_val_link(\n",
    "    ds_name=ds_name,\n",
    "    model_name=model_name,\n",
    "    range_triplets=range_triplets,\n",
    "    use_global_config_base=use_global_config_base,\n",
    "    densify=densify,\n",
    "    attr_opt=attr_opt,\n",
    "    test_p=test_p,\n",
    "    val_p=val_p,\n",
    "    n_reps=n_reps,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    verbose_in_funcs=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then perform 10 runs single run on the test set with parameters from the ds config\n",
    "#todo: make cross val link with global config \n",
    "\n",
    "#todo: change the auc dictionary to have the h@k from ogb as an accuracy\n",
    "#what are the small tasks here? also need to change to test scores and val scores folders. \n",
    "\n",
    "#how to handle two metrics on the same task? should i have different folders for each?\n",
    "\n",
    "#todo: folder structure: results/task/dataset/model/metric/validation and test \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    ds_name = 'ogbl-ddi'\n",
    "    model_name = 'ieclam'\n",
    "    val_dyads_to_omit = None\n",
    "    test_dyads_to_omit = None\n",
    "\n",
    "    ds = import_dataset(ds_name)\n",
    "    \n",
    "    if hasattr(ds, 'val_dyads_to_omit'):\n",
    "        val_dyads_to_omit = ds.val_dyads_to_omit\n",
    "    if hasattr(ds, 'test_dyads_to_omit'):\n",
    "        test_dyads_to_omit = ds.test_dyads_to_omit\n",
    "\n",
    "    # OMIT TEST\n",
    "    ds_omitted = ds.clone()\n",
    "    if test_dyads_to_omit is None: \n",
    "        ds_omitted.omitted_dyads_test, ds_omitted.edge_index, ds_omitted.edge_attr = lp.get_dyads_to_omit(\n",
    "                                            ds.edge_index, \n",
    "                                            ds.edge_attr, \n",
    "                                            test_p)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        assert type(test_dyads_to_omit) == tuple\n",
    "        assert utils.is_undirected(test_dyads_to_omit[0]) and utils.is_undirected(test_dyads_to_omit[1])\n",
    "        \n",
    "        ds_omitted.omitted_dyads_test, ds_omitted.edge_index, ds_omitted.edge_attr = lp.omit_dyads(ds_omitted.edge_index,\n",
    "                                    ds_omitted.edge_attr,\n",
    "                                    test_dyads_to_omit)\n",
    "        \n",
    "\n",
    "    if val_dyads_to_omit is None:\n",
    "        ds_test_val_omitted.omitted_dyads_val, ds_test_val_omitted.edge_index, ds_test_val_omitted.edge_attr = lp.get_dyads_to_omit(\n",
    "                                        ds_test_omitted.edge_index, \n",
    "                                        ds_test_omitted.edge_attr, \n",
    "                                        ((val_p)/(1-test_p)))# the amount to extract from the remaining edges to get the initial extraction we wanted for val (size changes after removal).\n",
    "\n",
    "    else:\n",
    "        #todo: if this condition holds also dont do the sampling at every iteration\n",
    "        assert type(val_dyads_to_omit) == tuple\n",
    "        assert utils.is_undirected(val_dyads_to_omit[0]) and utils.is_undirected(val_dyads_to_omit[1])\n",
    "\n",
    "        ds_test_omitted.omitted_dyads_val = val_dyads_to_omit\n",
    "        ds_test_omitted.omitted_dyads_val, ds_test_omitted.edge_index, ds_test_omitted.edge_attr = lp.omit_dyads(\n",
    "                        ds_test_omitted.edge_index, \n",
    "                        ds_test_omitted.edge_attr,\n",
    "                        val_dyads_to_omit)\n",
    "\n",
    "    for values in itertools.product(*[triplet[2] for triplet in range_triplets]):\n",
    "        for _ in range(n_reps): \n",
    "    \n",
    "            ds_test_val_omitted = ds_test_omitted.clone()\n",
    "            \n",
    "            # OMIT VALIDATION DYADS\n",
    "            '''edge attr signifies if the edge is omitted or not. if the edge_attr is 0 then the edge is an omitted dyad.'''\n",
    "\n",
    "            \n",
    "            # ============ OMIT VALIDATION =============\n",
    "\n",
    "            if densify:\n",
    "                ds_test_val_omitted.edge_index, ds_test_val_omitted.edge_attr = up.two_hop_link(ds_test_val_omitted)\n",
    "                                        \n",
    "            outers = []\n",
    "            inners = []\n",
    "            for i in range(len(range_triplets)):\n",
    "                outers.append(range_triplets[i][0])\n",
    "                inners.append(range_triplets[i][1])\n",
    "                \n",
    "            config_triplets = [\n",
    "                [outers[i], inners[i], values[i]] for i in range(len(range_triplets))\n",
    "                        ]\n",
    "            # if model_name in {'ieclam', 'pieclam'}:\n",
    "            #     if 's_reg' in inners:\n",
    "            #         ind_s = inners.index('s_reg')\n",
    "            #         config_triplets.append([outers[ind_s], inners[ind_s], values[ind_s]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            trainer = Trainer(\n",
    "                        dataset=ds_test_val_omitted,\n",
    "                        model_name=model_name,\n",
    "                        task='link_prediction',\n",
    "                        config_triplets_to_change=config_triplets,\n",
    "                        use_global_config_base=use_global_config_base,\n",
    "                        attr_opt=False,\n",
    "                        device=device,\n",
    "            )\n",
    "\n",
    "            losses, acc_test, acc_val = trainer.train(\n",
    "                        init_type='small_gaus',\n",
    "                        init_feats=True,\n",
    "                        acc_every=20,\n",
    "                        plot_every=plot_every,\n",
    "                        verbose=False,\n",
    "                        verbose_in_funcs=False\n",
    "                    )\n",
    "            \n",
    "            \n",
    "            if acc_test['auc']:\n",
    "                last_acc_test = acc_test['auc'][-1]\n",
    "            else:\n",
    "                last_acc_test = None\n",
    "            \n",
    "            if acc_val['auc']:\n",
    "                last_acc_val = acc_val['auc'][-1]                    \n",
    "            else:\n",
    "                last_acc_val = None\n",
    "            run_saver.update_file((last_acc_test, last_acc_val), config_triplets)\n",
    "        \n",
    "            del ds_test_val_omitted\n",
    "            ds_test_val_omitted = None\n",
    "            torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    if ds is not None:\n",
    "        del ds\n",
    "    if ds_test_omitted is not None:\n",
    "        del ds_test_omitted\n",
    "    if ds_test_val_omitted is not None:\n",
    "        del ds_test_val_omitted\n",
    "    torch.cuda.empty_cache()\n",
    "    printd('\\n\\nFinished CrossVal!\\n\\n')    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piegam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
